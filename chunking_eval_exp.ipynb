{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9042abbb",
   "metadata": {},
   "source": [
    "## 0. Cuda Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3faab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\ifanglab\\anaconda3\\envs\\rag\\lib\\site-packages (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0208f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118\n",
      "True\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8214685",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e91851",
   "metadata": {},
   "source": [
    "### 1.1 Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67019860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/brandonstarxel/chunking_evaluation.git --quiet\n",
    "%pip install hf_xet --quiet\n",
    "%pip install bitsandbytes --quiet\n",
    "%pip install --upgrade transformers --quiet\n",
    "%pip install --upgrade chromadb --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff2969",
   "metadata": {},
   "source": [
    "### 1.2 Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01aee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已將專案根目錄 'd:\\ArtificialIntelligenceCustomerService\\code\\exp' 優先加入到 Python 模組搜尋路徑。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 取得目前筆記本檔案的絕對路徑\n",
    "notebook_path = os.path.abspath('chunking_exp.ipynb')\n",
    "\n",
    "# 取得筆記本所在的目錄（即專案根目錄）\n",
    "project_root = os.path.dirname(notebook_path)\n",
    "\n",
    "# 將專案根目錄（包含你的本地 chunking_evaluation 資料夾）\n",
    "# 插入到 sys.path 的最前面，確保優先載入本地版本\n",
    "sys.path.insert(0, project_root)\n",
    "print(f\"已將專案根目錄 '{project_root}' 優先加入到 Python 模組搜尋路徑。\")\n",
    "\n",
    "\n",
    "from chunking_evaluation.chunking import FixedTokenChunker, RecursiveTokenChunker\n",
    "from chunking_evaluation import GeneralEvaluation, SyntheticEvaluation\n",
    "from chunking_evaluation.utils import bge_m3_token_count, get_bge_m3_embedding_function\n",
    "from chunking_evaluation.evaluation_framework.general_evaluation_data.DatasetAnalyzer import DatasetAnalyzer\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import http.client\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff1647",
   "metadata": {},
   "source": [
    "### 1.3 Setup Embedding Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9f8e5",
   "metadata": {},
   "source": [
    "## 2. Create Chunkers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bad7fd",
   "metadata": {},
   "source": [
    "### 2.1 RecursiveTokenChunker & FixedTokenChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0aa7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkers = [\n",
    "    # chunk_size = 512\n",
    "    RecursiveTokenChunker(chunk_size=512, chunk_overlap=50, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=512, chunk_overlap=50),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=512, chunk_overlap=100, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=512, chunk_overlap=100),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=512, chunk_overlap=150, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=512, chunk_overlap=150),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=512, chunk_overlap=200, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=512, chunk_overlap=200),\n",
    "    '''\n",
    "    # chunk_size = 1024\n",
    "    RecursiveTokenChunker(chunk_size=1024, chunk_overlap=50, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=1024, chunk_overlap=50),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=1024, chunk_overlap=100, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=1024, chunk_overlap=100),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=1024, chunk_overlap=150, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=1024, chunk_overlap=150),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=1024, chunk_overlap=200, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=1024, chunk_overlap=200),\n",
    "\n",
    "    # chunk_size = 2048\n",
    "    RecursiveTokenChunker(chunk_size=2048, chunk_overlap=50, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=2048, chunk_overlap=50),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=2048, chunk_overlap=100, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=2048, chunk_overlap=100),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=2048, chunk_overlap=150, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=2048, chunk_overlap=150),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=2048, chunk_overlap=200, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=2048, chunk_overlap=200),\n",
    "\n",
    "    # chunk_size = 4096\n",
    "    RecursiveTokenChunker(chunk_size=4096, chunk_overlap=50, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=4096, chunk_overlap=50),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=4096, chunk_overlap=100, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=4096, chunk_overlap=100),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=4096, chunk_overlap=150, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=4096, chunk_overlap=150),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=4096, chunk_overlap=200, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=4096, chunk_overlap=200),\n",
    "    '''\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b187",
   "metadata": {},
   "source": [
    "## 3. Prepare Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02754655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "跳過下載範例文本。\n"
     ]
    }
   ],
   "source": [
    "def download_text(book_id, file_name, directory):\n",
    "    conn = http.client.HTTPSConnection(\"www.gutenberg.org\")\n",
    "    url = f\"/files/{book_id}/{book_id}-0.txt\"\n",
    "\n",
    "    conn.request(\"GET\", url)\n",
    "    response = conn.getresponse()\n",
    "\n",
    "    if response.status == 200:\n",
    "        text = response.read().decode('utf-8')\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the text to the specified file within the directory\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text)\n",
    "        print(f\"Book '{file_name}' downloaded and saved successfully in '{directory}'.\")\n",
    "    else:\n",
    "        print(f\"Failed to download the book. Status code: {response.status}\")\n",
    "\n",
    "\n",
    "# Define the directory to save the books\n",
    "directory = \"corpora\"\n",
    "\n",
    "def download_example_texts(download=False, directory=\"./corpora\"):\n",
    "    if download:\n",
    "        books = {\n",
    "            1661: \"the_adventures_of_sherlock_holmes.txt\",\n",
    "            1342: \"pride_and_prejudice.txt\", \n",
    "            174: \"the_picture_of_dorian_gray.txt\"\n",
    "        }\n",
    "        print(\"開始下載範例文本...\")\n",
    "\n",
    "        for book_id, file_name in books.items():\n",
    "            try:\n",
    "                download_text(book_id, file_name, directory)\n",
    "                print(f\"✅ 成功下載: {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 下載失敗 {file_name}: {e}\")\n",
    "        \n",
    "        print(\"範例文本下載完成！\")\n",
    "    else:\n",
    "        print(\"跳過下載範例文本。\")\n",
    "\n",
    "download_example_texts(download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1aade",
   "metadata": {},
   "source": [
    "## 4. Initialize the Evaluation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b202f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Backup file './chunking_evaluation/evaluation_framework/general_evaluation_data/generated_queries_and_excerpts_backup.csv' already exists.\n",
      "📖 Reading file paths from the backup file.\n",
      "\n",
      "✅ The file paths to be processed have been saved in corpora_paths_list\n",
      "📁 The generated queries will be saved to: ./chunking_evaluation/evaluation_framework/general_evaluation_data/generated_queries_and_excerpts.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# Define your corpus folder path\n",
    "corpora_directory = \"./chunking_evaluation/evaluation_framework/general_evaluation_data/corpora\"\n",
    "# Define the path to check\n",
    "queries_backup_path = \"./chunking_evaluation/evaluation_framework/general_evaluation_data/generated_queries_and_excerpts_backup.csv\"\n",
    "# Define the target output path\n",
    "queries_csv_path = \"./chunking_evaluation/evaluation_framework/general_evaluation_data/generated_queries_and_excerpts.csv\"\n",
    "\n",
    "# --- Core logic modification ---\n",
    "corpora_paths_list = []\n",
    "\n",
    "# Check if the backup file exists\n",
    "if os.path.exists(queries_backup_path):\n",
    "    print(f\"✅ Backup file '{queries_backup_path}' already exists.\")\n",
    "    print(f\"📖 Reading file paths from the backup file.\")\n",
    "    \n",
    "    # Read the 'corpus_id' column from the CSV backup file\n",
    "    backup_df = pd.read_csv(queries_backup_path)\n",
    "    \n",
    "    # Standardize the paths and extract the unique list\n",
    "    corpora_paths_list = [os.path.normpath(path).replace('\\\\', '/') for path in backup_df['corpus_id'].unique()]\n",
    "else:\n",
    "    print(f\"🚧 Backup file '{queries_backup_path}' not found, starting sampling.\")\n",
    "\n",
    "    analyzer = DatasetAnalyzer(corpora_directory)\n",
    "    analyzer.analyze_folder()\n",
    "    analyzer.generate_report()\n",
    "    sample = analyzer.get_stratified_sample(100)\n",
    "\n",
    "    # Extract the file path list and ensure it's in a standardized format\n",
    "    corpora_paths_list = [os.path.normpath(file_info['file_path']).replace('\\\\', '/') for file_info in sample]\n",
    "\n",
    "    print(f\"\\n🎯 Stratified sampling complete, selected {len(sample)} files:\")\n",
    "    for i, file_info in enumerate(sample, 1):\n",
    "        print(f\"  {i:2d}. {file_info['category']} - {file_info['filename']}\")\n",
    "\n",
    "# --- Rest of the code remains the same ---\n",
    "print(f\"\\n✅ The file paths to be processed have been saved in corpora_paths_list\")\n",
    "print(f\"📁 The generated queries will be saved to: {queries_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131bb11",
   "metadata": {},
   "source": [
    "## 5. Generate Queries and Excerpts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bc6ea",
   "metadata": {},
   "source": [
    "### 5.1 Memory manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d054b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade ipywidgets --quiet\n",
    "%pip install accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3fb8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在預估模型分佈...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fb56c384f14109a1c382e90e6fc7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根據 max_memory={0: '12GiB', 'cpu': '8GiB'} 預估裝置分佈中...\n",
      "預估的裝置分佈: OrderedDict({'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 'cpu', 'model.layers.25': 'cpu', 'model.layers.26': 'cpu', 'model.layers.27': 'cpu', 'model.layers.28': 'cpu', 'model.layers.29': 'cpu', 'model.layers.30': 'cpu', 'model.layers.31': 'cpu', 'model.norm': 'cpu', 'model.rotary_emb': 'cpu', 'lm_head': 'cpu'})\n",
      "正在載入 meta-llama/Llama-3.1-8B-Instruct 模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c271c487343d5a72154893c266b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.1-8B-Instruct 模型已成功載入到裝置: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 'cpu', 'model.layers.25': 'cpu', 'model.layers.26': 'cpu', 'model.layers.27': 'cpu', 'model.layers.28': 'cpu', 'model.layers.29': 'cpu', 'model.layers.30': 'cpu', 'model.layers.31': 'cpu', 'model.norm': 'cpu', 'model.rotary_emb': 'cpu', 'lm_head': 'cpu'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the evaluation\n",
    "evaluation = SyntheticEvaluation(corpora_paths_list, queries_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc149eb",
   "metadata": {},
   "source": [
    "### 5.2 Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a19b652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 備份檔案 './chunking_evaluation/evaluation_framework/general_evaluation_data/generated_queries_and_excerpts_backup.csv' 已存在，跳過查詢生成步驟。\n"
     ]
    }
   ],
   "source": [
    "# Generate queries and excerpts, and save to CSV\n",
    "if os.path.exists(queries_backup_path):\n",
    "    print(f\"✅ 備份檔案 '{queries_backup_path}' 已存在，跳過查詢生成步驟。\")\n",
    "else:\n",
    "    print(f\"🚀 正在生成查詢與摘錄...\")\n",
    "    evaluation.generate_queries_and_excerpts(approximate_excerpts=True, num_rounds=1, queries_per_corpus=1)\n",
    "    print(f\"✔️ 查詢生成完成，已儲存至 '{queries_csv_path}'\")\n",
    "#evaluation.debug_full_output(\"./corpora/113_2_資訊工程學系_微積分(二)[159040]_陳榮銘.html\", use_approx=True, save_to_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0254ee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 開始修復 CSV 中的 references 欄位...\n",
      "🔧 已修正 'corpus_id' 欄位中的路徑格式。\n",
      "📊 讀取到 181 條記錄\n",
      "正在修復 references 欄位...\n",
      "✅ 修復完成!\n",
      "   成功修復: 181 條\n",
      "   修復失敗: 0 條\n",
      "   總計: 181 條\n",
      "🔍 驗證 CSV 文件完整性...\n",
      "❌ 驗證過程中發生錯誤: cannot access local variable 'df' where it is not associated with a value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.repair_csv_references() \n",
    "evaluation.validate_csv_integrity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc9303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to remove queries with poor excerpts\n",
    "#evaluation.filter_poor_excerpts(threshold=0.36)\n",
    "\n",
    "# Apply filter to remove duplicates\n",
    "#evaluation.filter_duplicates(threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e449e2",
   "metadata": {},
   "source": [
    "## 6. Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599e6be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to use the frozen embeddings originally used in the paper. As a result, this package will now generate a new set of embeddings. The change should be minimal and only come from the noise floor of SentenceTransformer's embedding function. The error:  Collection [auto_questions_bge_m3] does not exists\n",
      "Existing 'auto_questions' collection deleted.\n",
      "New 'auto_questions' collection created.\n",
      "181 questions added to 'auto_questions' collection.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "chroma_db_path = \"./chunking_evaluation/evaluation_framework/general_evaluation_data/questions_db\"\n",
    "\n",
    "# Initialize evaluation\n",
    "evaluation = GeneralEvaluation()\n",
    "\n",
    "results = []\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Display the DataFrame\n",
    "display_handle = display(df, display_id=True)\n",
    "\n",
    "for chunker in chunkers:\n",
    "    result = evaluation.run(chunker, get_bge_m3_embedding_function(), retrieve=5, db_to_save_chunks=chroma_db_path)\n",
    "    del result['corpora_scores']  \n",
    "    chunk_size = chunker._chunk_size if hasattr(chunker, '_chunk_size') else 0\n",
    "    chunk_overlap = chunker._chunk_overlap if hasattr(chunker, '_chunk_overlap') else 0\n",
    "    result['chunker'] = chunker.__class__.__name__ + f\"_{chunk_size}_{chunk_overlap}\"\n",
    "    results.append(result)\n",
    "\n",
    "    # Update the DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    clear_output(wait=True)\n",
    "    display_handle.update(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f09dc7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iou_mean</th>\n",
       "      <th>iou_std</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>precision_omega_mean</th>\n",
       "      <th>precision_omega_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>chunker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RecursiveTokenChunker_512_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FixedTokenChunker_512_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RecursiveTokenChunker_512_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FixedTokenChunker_512_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RecursiveTokenChunker_512_150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FixedTokenChunker_512_150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RecursiveTokenChunker_512_200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FixedTokenChunker_512_200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>str_0_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iou_mean  iou_std  recall_mean  recall_std  precision_omega_mean  \\\n",
       "0       0.0      0.0          0.0         0.0                   0.0   \n",
       "1       0.0      0.0          0.0         0.0                   0.0   \n",
       "2       0.0      0.0          0.0         0.0                   0.0   \n",
       "3       0.0      0.0          0.0         0.0                   0.0   \n",
       "4       0.0      0.0          0.0         0.0                   0.0   \n",
       "5       0.0      0.0          0.0         0.0                   0.0   \n",
       "6       0.0      0.0          0.0         0.0                   0.0   \n",
       "7       0.0      0.0          0.0         0.0                   0.0   \n",
       "8       0.0      0.0          0.0         0.0                   0.0   \n",
       "\n",
       "   precision_omega_std  precision_mean  precision_std  \\\n",
       "0                  0.0             0.0            0.0   \n",
       "1                  0.0             0.0            0.0   \n",
       "2                  0.0             0.0            0.0   \n",
       "3                  0.0             0.0            0.0   \n",
       "4                  0.0             0.0            0.0   \n",
       "5                  0.0             0.0            0.0   \n",
       "6                  0.0             0.0            0.0   \n",
       "7                  0.0             0.0            0.0   \n",
       "8                  0.0             0.0            0.0   \n",
       "\n",
       "                         chunker  \n",
       "0   RecursiveTokenChunker_512_50  \n",
       "1       FixedTokenChunker_512_50  \n",
       "2  RecursiveTokenChunker_512_100  \n",
       "3      FixedTokenChunker_512_100  \n",
       "4  RecursiveTokenChunker_512_150  \n",
       "5      FixedTokenChunker_512_150  \n",
       "6  RecursiveTokenChunker_512_200  \n",
       "7      FixedTokenChunker_512_200  \n",
       "8                        str_0_0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
