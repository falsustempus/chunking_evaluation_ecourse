{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9042abbb",
   "metadata": {},
   "source": [
    "## 0. Cuda Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3faab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8214685",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e91851",
   "metadata": {},
   "source": [
    "### 1.1 Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67019860",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/brandonstarxel/chunking_evaluation.git --quiet\n",
    "%pip install hf_xet --quiet\n",
    "%pip install bitsandbytes --quiet\n",
    "%pip install --upgrade transformers --quiet\n",
    "%pip install --upgrade chromadb --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff2969",
   "metadata": {},
   "source": [
    "### 1.2 Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01aee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# å–å¾—ç›®å‰ç­†è¨˜æœ¬æª”æ¡ˆçš„çµ•å°è·¯å¾‘\n",
    "notebook_path = os.path.abspath('chunking_exp.ipynb')\n",
    "\n",
    "# å–å¾—ç­†è¨˜æœ¬æ‰€åœ¨çš„ç›®éŒ„ï¼ˆå³å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰\n",
    "project_root = os.path.dirname(notebook_path)\n",
    "\n",
    "# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼ˆåŒ…å«ä½ çš„æœ¬åœ° chunking_evaluation è³‡æ–™å¤¾ï¼‰\n",
    "# æ’å…¥åˆ° sys.path çš„æœ€å‰é¢ï¼Œç¢ºä¿å„ªå…ˆè¼‰å…¥æœ¬åœ°ç‰ˆæœ¬\n",
    "sys.path.insert(0, project_root)\n",
    "print(f\"å·²å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„ '{project_root}' å„ªå…ˆåŠ å…¥åˆ° Python æ¨¡çµ„æœå°‹è·¯å¾‘ã€‚\")\n",
    "\n",
    "\n",
    "from chunking_evaluation.chunking import FixedTokenChunker, RecursiveTokenChunker\n",
    "from chunking_evaluation import GeneralEvaluation, SyntheticEvaluation\n",
    "from chunking_evaluation.utils import bge_m3_token_count, get_bge_m3_embedding_function\n",
    "from chunking_evaluation.evaluation_framework.general_evaluation_data.DatasetAnalyzer import DatasetAnalyzer\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import http.client\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff1647",
   "metadata": {},
   "source": [
    "### 1.3 Setup Embedding Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9f8e5",
   "metadata": {},
   "source": [
    "## 2. Create Chunkers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bad7fd",
   "metadata": {},
   "source": [
    "### 2.1 RecursiveTokenChunker & FixedTokenChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0aa7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkers = [\n",
    "    # chunk_size = 512\n",
    "    RecursiveTokenChunker(chunk_size=512, chunk_overlap=50, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=512, chunk_overlap=50),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=512, chunk_overlap=100, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=512, chunk_overlap=100),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=512, chunk_overlap=150, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=512, chunk_overlap=150),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=512, chunk_overlap=200, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=512, chunk_overlap=200),\n",
    "    '''\n",
    "    # chunk_size = 1024\n",
    "    RecursiveTokenChunker(chunk_size=1024, chunk_overlap=50, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=1024, chunk_overlap=50),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=1024, chunk_overlap=100, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=1024, chunk_overlap=100),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=1024, chunk_overlap=150, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=1024, chunk_overlap=150),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=1024, chunk_overlap=200, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=1024, chunk_overlap=200),\n",
    "\n",
    "    # chunk_size = 2048\n",
    "    RecursiveTokenChunker(chunk_size=2048, chunk_overlap=50, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=2048, chunk_overlap=50),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=2048, chunk_overlap=100, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=2048, chunk_overlap=100),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=2048, chunk_overlap=150, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=2048, chunk_overlap=150),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=2048, chunk_overlap=200, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=2048, chunk_overlap=200),\n",
    "\n",
    "    # chunk_size = 4096\n",
    "    RecursiveTokenChunker(chunk_size=4096, chunk_overlap=50, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=4096, chunk_overlap=50),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=4096, chunk_overlap=100, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=4096, chunk_overlap=100),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=4096, chunk_overlap=150, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=4096, chunk_overlap=150),\n",
    "\n",
    "    RecursiveTokenChunker(chunk_size=4096, chunk_overlap=200, length_function=bge_m3_token_count),\n",
    "    FixedTokenChunker(chunk_size=4096, chunk_overlap=200),\n",
    "    '''\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b187",
   "metadata": {},
   "source": [
    "## 3. Prepare Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02754655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_text(book_id, file_name, directory):\n",
    "    conn = http.client.HTTPSConnection(\"www.gutenberg.org\")\n",
    "    url = f\"/files/{book_id}/{book_id}-0.txt\"\n",
    "\n",
    "    conn.request(\"GET\", url)\n",
    "    response = conn.getresponse()\n",
    "\n",
    "    if response.status == 200:\n",
    "        text = response.read().decode('utf-8')\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the text to the specified file within the directory\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text)\n",
    "        print(f\"Book '{file_name}' downloaded and saved successfully in '{directory}'.\")\n",
    "    else:\n",
    "        print(f\"Failed to download the book. Status code: {response.status}\")\n",
    "\n",
    "\n",
    "# Define the directory to save the books\n",
    "directory = \"corpora\"\n",
    "\n",
    "def download_example_texts(download=False, directory=\"./corpora\"):\n",
    "    if download:\n",
    "        books = {\n",
    "            1661: \"the_adventures_of_sherlock_holmes.txt\",\n",
    "            1342: \"pride_and_prejudice.txt\", \n",
    "            174: \"the_picture_of_dorian_gray.txt\"\n",
    "        }\n",
    "        print(\"é–‹å§‹ä¸‹è¼‰ç¯„ä¾‹æ–‡æœ¬...\")\n",
    "\n",
    "        for book_id, file_name in books.items():\n",
    "            try:\n",
    "                download_text(book_id, file_name, directory)\n",
    "                print(f\"âœ… æˆåŠŸä¸‹è¼‰: {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ä¸‹è¼‰å¤±æ•— {file_name}: {e}\")\n",
    "        \n",
    "        print(\"ç¯„ä¾‹æ–‡æœ¬ä¸‹è¼‰å®Œæˆï¼\")\n",
    "    else:\n",
    "        print(\"è·³éä¸‹è¼‰ç¯„ä¾‹æ–‡æœ¬ã€‚\")\n",
    "\n",
    "download_example_texts(download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1aade",
   "metadata": {},
   "source": [
    "## 4. Initialize the Evaluation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# Define your corpus folder path\n",
    "corpora_directory = \"./chunking_evaluation/evaluation_framework/general_evaluation_data/corpora\"\n",
    "# Define the path to check\n",
    "queries_backup_path = \"./chunking_evaluation/evaluation_framework/general_evaluation_data/generated_queries_and_excerpts_backup.csv\"\n",
    "# Define the target output path\n",
    "queries_csv_path = \"./chunking_evaluation/evaluation_framework/general_evaluation_data/generated_queries_and_excerpts.csv\"\n",
    "\n",
    "# --- Core logic modification ---\n",
    "corpora_paths_list = []\n",
    "\n",
    "# Check if the backup file exists\n",
    "if os.path.exists(queries_backup_path):\n",
    "    print(f\"âœ… Backup file '{queries_backup_path}' already exists.\")\n",
    "    print(f\"ğŸ“– Reading file paths from the backup file.\")\n",
    "    \n",
    "    # Read the 'corpus_id' column from the CSV backup file\n",
    "    backup_df = pd.read_csv(queries_backup_path)\n",
    "    \n",
    "    # Standardize the paths and extract the unique list\n",
    "    corpora_paths_list = [os.path.normpath(path).replace('\\\\', '/') for path in backup_df['corpus_id'].unique()]\n",
    "else:\n",
    "    print(f\"ğŸš§ Backup file '{queries_backup_path}' not found, starting sampling.\")\n",
    "\n",
    "    analyzer = DatasetAnalyzer(corpora_directory)\n",
    "    analyzer.analyze_folder()\n",
    "    analyzer.generate_report()\n",
    "    sample = analyzer.get_stratified_sample(100)\n",
    "\n",
    "    # Extract the file path list and ensure it's in a standardized format\n",
    "    corpora_paths_list = [os.path.normpath(file_info['file_path']).replace('\\\\', '/') for file_info in sample]\n",
    "\n",
    "    print(f\"\\nğŸ¯ Stratified sampling complete, selected {len(sample)} files:\")\n",
    "    for i, file_info in enumerate(sample, 1):\n",
    "        print(f\"  {i:2d}. {file_info['category']} - {file_info['filename']}\")\n",
    "\n",
    "# --- Rest of the code remains the same ---\n",
    "print(f\"\\nâœ… The file paths to be processed have been saved in corpora_paths_list\")\n",
    "print(f\"ğŸ“ The generated queries will be saved to: {queries_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131bb11",
   "metadata": {},
   "source": [
    "## 5. Generate Queries and Excerpts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bc6ea",
   "metadata": {},
   "source": [
    "### 5.1 Memory manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d054b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade ipywidgets --quiet\n",
    "%pip install accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"corpora_paths_list:\", corpora_paths_list)\n",
    "corpora_id_paths = {\n",
    "    os.path.splitext(os.path.basename(path))[0]: path \n",
    "    for path in corpora_paths_list\n",
    "}\n",
    "\n",
    "print(\"Corpora ID Paths:\", corpora_id_paths)\n",
    "\n",
    "# Initialize the evaluation\n",
    "evaluation = SyntheticEvaluation(corpora_id_paths, queries_csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc149eb",
   "metadata": {},
   "source": [
    "### 5.2 Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate queries and excerpts, and save to CSV\n",
    "if os.path.exists(queries_backup_path):\n",
    "    print(f\"âœ… å‚™ä»½æª”æ¡ˆ '{queries_backup_path}' å·²å­˜åœ¨ï¼Œè·³éæŸ¥è©¢ç”Ÿæˆæ­¥é©Ÿã€‚\")\n",
    "else:\n",
    "    print(f\"ğŸš€ æ­£åœ¨ç”ŸæˆæŸ¥è©¢èˆ‡æ‘˜éŒ„...\")\n",
    "    evaluation.generate_queries_and_excerpts(approximate_excerpts=True, num_rounds=1, queries_per_corpus=1)\n",
    "    print(f\"âœ”ï¸ æŸ¥è©¢ç”Ÿæˆå®Œæˆï¼Œå·²å„²å­˜è‡³ '{queries_csv_path}'\")\n",
    "    evaluation.repair_csv_references() \n",
    "    evaluation.validate_csv_integrity()\n",
    "#evaluation.debug_full_output(\"./corpora/113_2_è³‡è¨Šå·¥ç¨‹å­¸ç³»_å¾®ç©åˆ†(äºŒ)[159040]_é™³æ¦®éŠ˜.html\", use_approx=True, save_to_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc9303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to remove queries with poor excerpts\n",
    "#evaluation.filter_poor_excerpts(threshold=0.36)\n",
    "\n",
    "# Apply filter to remove duplicates\n",
    "#evaluation.filter_duplicates(threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e449e2",
   "metadata": {},
   "source": [
    "## 6. Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "chroma_db_path = \"./chunking_evaluation/evaluation_framework/general_evaluation_data/questions_db\"\n",
    "\n",
    "\n",
    "first_chunker = chunkers[0]\n",
    "first_result = evaluation.run(first_chunker, get_bge_m3_embedding_function(), retrieve=5, db_to_save_chunks=chroma_db_path)\n",
    "print(first_result)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Initialize evaluation\n",
    "evaluation = GeneralEvaluation()\n",
    "\n",
    "results = []\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Display the DataFrame\n",
    "display_handle = display(df, display_id=True)\n",
    "\n",
    "for chunker in chunkers:\n",
    "    result = evaluation.run(chunker, get_bge_m3_embedding_function(), retrieve=5, db_to_save_chunks=chroma_db_path)\n",
    "    del result['corpora_scores']  \n",
    "    chunk_size = chunker._chunk_size if hasattr(chunker, '_chunk_size') else 0\n",
    "    chunk_overlap = chunker._chunk_overlap if hasattr(chunker, '_chunk_overlap') else 0\n",
    "    result['chunker'] = chunker.__class__.__name__ + f\"_{chunk_size}_{chunk_overlap}\"\n",
    "    results.append(result)\n",
    "\n",
    "    # Update the DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    clear_output(wait=True)\n",
    "    display_handle.update(df)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
