from typing import List, Dict, Any
import os
import json
import random
import re

from bs4 import BeautifulSoup
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from accelerate import infer_auto_device_map, init_empty_weights

from chromadb import Client, Settings
from chromadb.utils import embedding_functions

from chunking_evaluation.utils import rigorous_document_search, get_bge_m3_embedding_function
from .base_evaluation import BaseEvaluation

import pandas as pd
import numpy as np
from importlib import resources

class SyntheticEvaluation(BaseEvaluation):
    def __init__(self, corpora_paths: List[str], queries_csv_path: str, chroma_db_path: str = None):
        super().__init__(questions_csv_path=queries_csv_path, chroma_db_path=chroma_db_path)
        self.corpora_paths = corpora_paths
        self.questions_csv_path = queries_csv_path

        llama_model_id = "meta-llama/Llama-3.1-8B-Instruct"

        # å®šç¾©é‡åŒ–é…ç½®
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            llm_int8_enable_fp32_cpu_offload=True
        )

        try:
            # è¼‰å…¥åˆ†è©å™¨
            self.llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_id)
            if self.llama_tokenizer.pad_token is None:
                self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token

            # ä½¿ç”¨ accelerate é å…ˆè¨ˆç®—è£ç½®åˆ†ä½ˆï¼Œä¸¦çµ¦å‡ºé€²åº¦æ¢
            print("æ­£åœ¨é ä¼°æ¨¡å‹åˆ†ä½ˆ...")
            with init_empty_weights():
                dummy_model = AutoModelForCausalLM.from_pretrained(
                    llama_model_id,
                    torch_dtype=torch.bfloat16,
                    trust_remote_code=True
                )
            
            # æ ¹æ“šæ‚¨çš„ç³»çµ±èª¿æ•´é€™äº›å€¼ã€‚é€™æ˜¯ä¸€å€‹ç¤ºä¾‹é…ç½®ã€‚
            # å¦‚æœæ‚¨æœ‰å–®å¼µ 24GB VRAM çš„ GPUï¼Œæ‚¨å¯ä»¥å°‡å…¶è¨­å®šç‚º {0: "22GiB"}ã€‚
            max_memory = {0: "12GiB", "cpu": "8GiB"}
            print(f"æ ¹æ“š max_memory={max_memory} é ä¼°è£ç½®åˆ†ä½ˆä¸­...")
            device_map = infer_auto_device_map(
                dummy_model,
                max_memory=max_memory,
                no_split_module_classes=["LlamaDecoderLayer"]
            )
            print(f"é ä¼°çš„è£ç½®åˆ†ä½ˆ: {device_map}")
            
            # è¼‰å…¥æ¨¡å‹ï¼Œä¸¦ä½¿ç”¨ä¸Šé¢è¨ˆç®—çš„è£ç½®åˆ†ä½ˆ
            print(f"æ­£åœ¨è¼‰å…¥ {llama_model_id} æ¨¡å‹...")
            self.llama_model = AutoModelForCausalLM.from_pretrained(
                llama_model_id,
                device_map=device_map,
                quantization_config=quantization_config,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            print(f"{llama_model_id} æ¨¡å‹å·²æˆåŠŸè¼‰å…¥åˆ°è£ç½®: {self.llama_model.hf_device_map}")
        
        except Exception as e:
            print("=" * 50)
            print("âŒ è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”Ÿè‡´å‘½éŒ¯èª¤ âŒ")
            print("é€™é€šå¸¸æ˜¯å› ç‚ºè¨˜æ†¶é«”ä¸è¶³ã€‚è«‹å˜—è©¦ä»¥ä¸‹æ–¹æ³•ï¼š")
            print("1. è¼‰å…¥æ›´å°çš„æ¨¡å‹ï¼Œä¾‹å¦‚ `meta-llama/Llama-3.1-8B-Instruct`ã€‚")
            print("2. ç¢ºä¿æ‚¨æœ‰è¶³å¤ çš„ GPU VRAM å’Œç³»çµ± RAMã€‚")
            print(f"åŸå§‹éŒ¯èª¤è¨Šæ¯: {e}")
            print("=" * 50)
            # åœ¨é€™è£¡é¸æ“‡æ˜¯å¦è¦è®“ç¨‹å¼åœæ­¢ï¼Œæˆ–æ˜¯è¼‰å…¥ä¸€å€‹å‚™ç”¨æ¨¡å‹
            raise e
        
        self.synth_questions_df = None

        with resources.as_file(resources.files('chunking_evaluation.evaluation_framework') / 'prompts') as prompt_path:
            with open(os.path.join(prompt_path, 'question_maker_system.txt'), 'r') as f:
                self.question_maker_system_prompt = f.read()

            with open(os.path.join(prompt_path, 'question_maker_approx_system.txt'), 'r') as f:
                self.question_maker_approx_system_prompt = f.read()
            
            with open(os.path.join(prompt_path, 'question_maker_user.txt'), 'r') as f:
                self.question_maker_user_prompt = f.read()

            with open(os.path.join(prompt_path, 'question_maker_approx_user.txt'), 'r') as f:
                self.question_maker_approx_user_prompt = f.read()

    def _safe_json_loads(self, json_str):
        """
        å®‰å…¨çš„ JSON è§£æå‡½æ•¸ï¼Œè™•ç†å¯èƒ½çš„é›™é‡åºåˆ—åŒ–å•é¡Œ
        """
        if pd.isna(json_str) or json_str == '':
            return []
        
        try:
            # ç¬¬ä¸€æ¬¡è§£æ
            result = json.loads(json_str)
            
            # å¦‚æœçµæœæ˜¯å­—ä¸²ï¼Œèªªæ˜æœ‰é›™é‡åºåˆ—åŒ–ï¼Œéœ€è¦å†è§£æä¸€æ¬¡
            if isinstance(result, str):
                print("æª¢æ¸¬åˆ°é›™é‡åºåˆ—åŒ–ï¼Œé€²è¡Œç¬¬äºŒæ¬¡è§£æ...")
                result = json.loads(result)
            
            # ç¢ºä¿çµæœæ˜¯ list
            if not isinstance(result, list):
                print(f"è­¦å‘Šï¼šreferences ä¸æ˜¯ list æ ¼å¼ï¼Œè€Œæ˜¯ {type(result)}ï¼Œå˜—è©¦è½‰æ›...")
                if isinstance(result, dict):
                    result = [result]
                else:
                    raise ValueError(f"ç„¡æ³•è™•ç†çš„ references æ ¼å¼ï¼š{type(result)}")
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"JSON è§£æéŒ¯èª¤ï¼š{e}")
            print(f"åŸå§‹å­—ä¸²ï¼š{json_str[:200]}...")
            return []
        except Exception as e:
            print(f"å…¶ä»–è§£æéŒ¯èª¤ï¼š{e}")
            return []

    def _save_questions_df(self):
        """
        ä¿®æ­£çš„å„²å­˜å‡½æ•¸ï¼Œé¿å…é›™é‡åºåˆ—åŒ–
        """
        # å‰µå»ºä¸€å€‹å‰¯æœ¬ç”¨æ–¼å„²å­˜
        df_to_save = self.synth_questions_df.copy()
        
        # ç¢ºä¿ references æ¬„ä½æ­£ç¢ºåºåˆ—åŒ–
        def safe_serialize_references(refs):
            if isinstance(refs, str):
                # å¦‚æœå·²ç¶“æ˜¯å­—ä¸²ï¼Œå…ˆå˜—è©¦è§£æå†é‡æ–°åºåˆ—åŒ–
                try:
                    parsed_refs = self._safe_json_loads(refs)
                    return json.dumps(parsed_refs, ensure_ascii=False)
                except:
                    return refs
            elif isinstance(refs, (list, dict)):
                # å¦‚æœæ˜¯ç‰©ä»¶ï¼Œç›´æ¥åºåˆ—åŒ–
                return json.dumps(refs, ensure_ascii=False)
            else:
                # å…¶ä»–æƒ…æ³ï¼Œè½‰ç‚ºç©º list
                return json.dumps([], ensure_ascii=False)
        
        df_to_save['references'] = df_to_save['references'].apply(safe_serialize_references)
        df_to_save.to_csv(self.questions_csv_path, index=False)
        
    def _clean_html(self, html_content: str) -> str:
        """ä½¿ç”¨ BeautifulSoup æ¸…ç† HTML å…§å®¹ï¼Œæå–ç´”æ–‡æœ¬ã€‚"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for script_or_style in soup(['script', 'style']):
                script_or_style.decompose()
            clean_text = soup.get_text()
            clean_text = ' '.join(clean_text.split())
            return clean_text
        except Exception as e:
            print(f"HTML æ¸…ç†å¤±æ•—: {e}")
            return html_content

    def _clean_text(self, text_content: str) -> str:
        """ç°¡å–®æ¸…ç†ç´”æ–‡æœ¬å…§å®¹ï¼Œç§»é™¤å¤šé¤˜ç©ºç™½ã€‚"""
        return re.sub(r'\s+', ' ', text_content).strip()

    def _get_cleaned_document_content(self, file_path: str) -> str:
        """æ ¹æ“šæª”æ¡ˆå‰¯æª”åï¼Œè®€å–ä¸¦æ¸…ç†æ–‡ä»¶å…§å®¹ã€‚"""
        file_extension = os.path.splitext(file_path)[1].lower()
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_content = f.read()
        except UnicodeDecodeError:
            print(f"å˜—è©¦ä»¥ latin-1 è®€å–æª”æ¡ˆ: {file_path}")
            with open(file_path, 'r', encoding='latin-1') as f:
                raw_content = f.read()
        except Exception as e:
            print(f"è®€å–æª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return ""

        if file_extension in ['.html', '.htm']:
            print(f" - æ­£åœ¨æ¸…ç† HTML æª”æ¡ˆ: {file_path}")
            return self._clean_html(raw_content)
        elif file_extension in ['.txt', '.md', '.markdown']:
            print(f" - æ­£åœ¨æ¸…ç† TEXT/MARKDOWN æª”æ¡ˆ: {file_path}")
            return self._clean_text(raw_content)
        else:
            print(f" - æœªçŸ¥æª”æ¡ˆé¡å‹ ({file_extension})ï¼Œé€²è¡Œé€šç”¨æ¸…ç†: {file_path}")
            return self._clean_text(raw_content)

    def _tag_text(self, text):
        chunk_length = 100
        chunks = []
        tag_indexes = [0]
        start = 0
        while start < len(text):
            end = start + chunk_length
            chunk = text[start:end]
            if end < len(text):
                space_index = chunk.rfind(' ')
                if space_index != -1:
                    end = start + space_index + 1
                    chunk = text[start:end]
            chunks.append(chunk)
            tag_indexes.append(end)
            start = end
        tagged_text = ""
        for i, chunk in enumerate(chunks):
            tagged_text += f"<start_chunk_{i}>" + chunk + f"<end_chunk_{i}>"
        return tagged_text, tag_indexes
    
    def _run_llama_inference(self, system_prompt: str, user_prompt: str, max_tokens: int) -> str:
        """
        æ”¹é€²çš„ Llama æ¨ç†å‡½å¼ï¼Œä½¿ç”¨æ›´å¯é çš„å›æ‡‰æå–é‚è¼¯
        """
        
        # Llama 3.1 çš„ chat æ ¼å¼
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # æ‡‰ç”¨ chat template
        prompt = self.llama_tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # åŠ å¼·çš„ JSON æ ¼å¼æç¤º
        json_instruction = "\n\nIMPORTANT: You must respond with ONLY a valid JSON object. Do not include any explanations, comments, or additional text outside the JSON structure."
        full_prompt = prompt + json_instruction
        
        # è¨˜éŒ„åŸå§‹ prompt çš„é•·åº¦ï¼Œç”¨æ–¼å¾ŒçºŒç²¾ç¢ºåˆ‡å‰²
        self._original_prompt = full_prompt
        
        inputs = self.llama_tokenizer(
            full_prompt, 
            return_tensors="pt", 
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.llama_model.device)
        
        with torch.no_grad():
            outputs = self.llama_model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.1,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.05,
                pad_token_id=self.llama_tokenizer.eos_token_id,
                eos_token_id=self.llama_tokenizer.eos_token_id,
                use_cache=True
            )
        
        # è§£ç¢¼å®Œæ•´è¼¸å‡º
        full_response = self.llama_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ä½¿ç”¨æ”¹é€²çš„å›æ‡‰æå–é‚è¼¯
        model_response = self._extract_model_response_from_full_output(full_response, full_prompt)
        
        return model_response

    def _extract_model_response_from_full_output(self, full_response: str, original_prompt: str) -> str:
        """
        æ”¹é€²çš„æ¨¡å‹å›æ‡‰æå–é‚è¼¯ï¼Œæ›´å¯é åœ°åˆ†é›¢æ¨¡å‹çš„å¯¦éš›å›æ‡‰
        """
        # ç­–ç•¥1: ç²¾ç¢ºå‰ç¶´åŒ¹é…
        if full_response.startswith(original_prompt):
            model_response = full_response[len(original_prompt):].strip()
            if model_response:
                print("âœ… æˆåŠŸä½¿ç”¨ç²¾ç¢ºå‰ç¶´åŒ¹é…æå–å›æ‡‰")
                return model_response

        # ç­–ç•¥2: å°‹æ‰¾ assistant æ¨™è¨˜ï¼ˆé©ç”¨æ–¼ Llama chat formatï¼‰
        assistant_markers = [
            "<|start_header_id|>assistant<|end_header_id|>\n\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "<|start_header_id|>assistant<|end_header_id|>",
            "assistant<|end_header_id|>\n\n",
            "assistant<|end_header_id|>\n",
            "assistant<|end_header_id|>",
        ]
        
        for marker in assistant_markers:
            if marker in full_response:
                # æ‰¾åˆ°æœ€å¾Œä¸€å€‹ assistant æ¨™è¨˜ï¼ˆé¿å…æå–ç¯„ä¾‹ï¼‰
                last_pos = full_response.rfind(marker)
                if last_pos != -1:
                    response_start = last_pos + len(marker)
                    model_response = full_response[response_start:].strip()
                    if model_response and not self._contains_example_content(model_response):
                        print(f"âœ… æˆåŠŸä½¿ç”¨ assistant æ¨™è¨˜æå–å›æ‡‰: {marker}")
                        return model_response

        # ç­–ç•¥3: å°‹æ‰¾ JSON æŒ‡ä»¤å¾Œçš„ç¬¬ä¸€å€‹çœŸå¯¦ JSONï¼ˆé¿é–‹ç¯„ä¾‹ï¼‰
        json_instruction_variants = [
            "IMPORTANT: You must respond with ONLY a valid JSON object",
            "You must respond with ONLY a valid JSON object",
            "respond with ONLY a valid JSON object"
        ]
        
        for instruction in json_instruction_variants:
            instruction_pos = full_response.find(instruction)
            if instruction_pos != -1:
                # å¾æŒ‡ä»¤å¾Œé–‹å§‹æœç´¢
                search_start = instruction_pos + len(instruction)
                potential_jsons = self._find_all_json_objects(full_response[search_start:])
                
                # éæ¿¾æ‰åŒ…å«ç¯„ä¾‹å…§å®¹çš„ JSON
                for json_obj in potential_jsons:
                    if not self._contains_example_content(json_obj):
                        print("âœ… æˆåŠŸä½¿ç”¨ JSON æŒ‡ä»¤å¾Œå®šä½æå–å›æ‡‰")
                        return json_obj

        # ç­–ç•¥4: å°‹æ‰¾æœ€å¾Œä¸€å€‹å®Œæ•´çš„ JSON å°è±¡ï¼ˆæ’é™¤ç¯„ä¾‹ï¼‰
        all_jsons = self._find_all_json_objects(full_response)
        for json_obj in reversed(all_jsons):  # å¾å¾Œå¾€å‰æ‰¾
            if not self._contains_example_content(json_obj):
                print("âœ… ä½¿ç”¨æœ€å¾Œ JSON å°è±¡æå–å›æ‡‰")
                return json_obj

        # ç­–ç•¥5: å¦‚æœä»¥ä¸Šéƒ½å¤±æ•—ï¼Œå˜—è©¦æ‰¾åˆ°ä¸åŒ…å«ç¯„ä¾‹çš„ä»»ä½• JSON
        for json_obj in all_jsons:
            if not self._contains_example_content(json_obj):
                print("âš ï¸ ä½¿ç”¨ä»»æ„éç¯„ä¾‹ JSON å°è±¡æå–å›æ‡‰")
                return json_obj

        # æœ€å¾Œæ‰‹æ®µï¼šè¿”å›åŸå§‹å›æ‡‰
        print("âŒ æ‰€æœ‰æå–ç­–ç•¥å¤±æ•—ï¼Œè¿”å›åŸå§‹å›æ‡‰")
        return full_response

    def _contains_example_content(self, text: str) -> bool:
        """
        æª¢æ¸¬æ–‡æœ¬æ˜¯å¦åŒ…å«ç¯„ä¾‹å…§å®¹ï¼Œé‡å°èª²ç¨‹å¤§ç¶±å„ªåŒ–
        """
        # æ›´æ–°ç‚ºèª²ç¨‹å¤§ç¶±ç›¸é—œçš„ç¯„ä¾‹é—œéµè©
        example_indicators = [
            "è³‡æ–™çµæ§‹èª²ç¨‹",  # ä¾†è‡ªæ–°ç¯„ä¾‹
            "å¼µæ˜è¯æ•™æˆ",    # ä¾†è‡ªæ–°ç¯„ä¾‹
            "CS201",         # ä¾†è‡ªæ–°ç¯„ä¾‹
            "Experiment A",  # ä¿ç•™èˆŠç¯„ä¾‹æª¢æ¸¬
            "Experiment B", 
            "Experiment C",
            "Experiment D",
            "é€™ç¯‡è«–æ–‡é€²è¡Œäº†å“ªäº›å¯¦é©—",  # èˆŠç¯„ä¾‹
            "temperature control test",
            "pH sensitivity test",
            "enzyme activity assay",
            "light exposure trial",
            "reaction rate",
            "UV light",
            "degradation of reactants"
        ]
        
        text_lower = text.lower()
        for indicator in example_indicators:
            if indicator.lower() in text_lower:
                return True
        return False

    def _find_all_json_objects(self, text: str) -> List[str]:
        """
        æ‰¾åˆ°æ–‡æœ¬ä¸­æ‰€æœ‰å®Œæ•´çš„ JSON å°è±¡
        """
        json_objects = []
        i = 0
        
        while i < len(text):
            # å°‹æ‰¾ JSON é–‹å§‹
            start_pos = text.find('{', i)
            if start_pos == -1:
                break
            
            # å°‹æ‰¾åŒ¹é…çš„çµæŸæ‹¬è™Ÿ
            brace_count = 0
            end_pos = -1
            
            for j in range(start_pos, len(text)):
                if text[j] == '{':
                    brace_count += 1
                elif text[j] == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end_pos = j + 1
                        break
            
            if end_pos != -1:
                json_candidate = text[start_pos:end_pos]
                json_objects.append(json_candidate)
                i = end_pos
            else:
                i = start_pos + 1
        
        return json_objects

    def _extract_json_from_response(self, response_text: str) -> dict:
        """
        æ”¹é€²çš„ JSON è§£æå™¨ï¼Œæ”¯æŒå¤šç¨® JSON æå–ç­–ç•¥
        """
        import re
        import json
        
        print(f"ğŸ“¥ é–‹å§‹è§£æå›æ‡‰ï¼Œé•·åº¦: {len(response_text)} å­—ç¬¦")
        
        # é¦–å…ˆæª¢æŸ¥æ˜¯å¦åŒ…å«ç¯„ä¾‹å…§å®¹
        if self._contains_example_content(response_text):
            print("âš ï¸ è­¦å‘Šï¼šæª¢æ¸¬åˆ°å›æ‡‰åŒ…å«ç¯„ä¾‹å…§å®¹ï¼Œé€™å¯èƒ½è¡¨ç¤ºå›æ‡‰æå–æœ‰å•é¡Œ")
        
        # 1. å˜—è©¦æ‰¾åˆ°æ‰€æœ‰ JSON å°è±¡
        json_candidates = self._find_all_json_objects(response_text)
        print(f"ğŸ“‹ æ‰¾åˆ° {len(json_candidates)} å€‹ JSON å€™é¸å°è±¡")
        
        # 2. æŒ‰å„ªå…ˆç´šå˜—è©¦è§£ææ¯å€‹å€™é¸å°è±¡
        for i, json_str in enumerate(json_candidates):
            print(f"ğŸ§ª å˜—è©¦è§£æå€™é¸å°è±¡ {i+1}: é•·åº¦ {len(json_str)} å­—ç¬¦")
            
            # è·³éæ˜é¡¯åŒ…å«ç¯„ä¾‹çš„ JSON
            if self._contains_example_content(json_str):
                print(f"â­ï¸ è·³éå€™é¸å°è±¡ {i+1}: åŒ…å«ç¯„ä¾‹å…§å®¹")
                continue
            
            try:
                result = self._parse_and_validate_json(json_str)
                if result:
                    print(f"âœ… æˆåŠŸè§£æå€™é¸å°è±¡ {i+1}")
                    return result
            except Exception as e:
                print(f"âŒ å€™é¸å°è±¡ {i+1} è§£æå¤±æ•—: {e}")
                continue
        
        # 3. å¦‚æœç›´æ¥è§£æå¤±æ•—ï¼Œå˜—è©¦ JSON ä¿®å¾©
        print("ğŸ”§ å˜—è©¦ JSON ä¿®å¾©ç­–ç•¥...")
        for i, json_str in enumerate(json_candidates):
            if self._contains_example_content(json_str):
                continue
                
            try:
                fixed_json = self._fix_common_json_issues(json_str)
                result = self._parse_and_validate_json(fixed_json)
                if result:
                    print(f"âœ… ä¿®å¾©å¾ŒæˆåŠŸè§£æå€™é¸å°è±¡ {i+1}")
                    return result
            except Exception as e:
                continue
        
        # 4. æœ€å¾Œæ‰‹æ®µï¼šå˜—è©¦å¾å›æ‡‰ä¸­æå–å¯èƒ½çš„ JSON ç‰‡æ®µ
        print("ğŸš¨ ä½¿ç”¨æœ€å¾Œæ‰‹æ®µï¼šæå– JSON ç‰‡æ®µ")
        try:
            return self._extract_json_fragments(response_text)
        except Exception as e:
            raise ValueError(f"æ‰€æœ‰ JSON è§£æç­–ç•¥å‡å¤±æ•—ã€‚åŸå§‹éŒ¯èª¤: {e}")

    def _parse_and_validate_json(self, json_str: str) -> dict:
        """
        è§£æå’Œé©—è­‰ JSON å°è±¡
        """
        import json
        
        result = json.loads(json_str)
        
        # é©—è­‰å¿…éœ€å­—æ®µ
        required_fields = ['question', 'references']
        for field in required_fields:
            if field not in result:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # é©—è­‰ question ä¸ç‚ºç©ºä¸”ä¸æ˜¯ç¯„ä¾‹
        question = result['question']
        if not question or len(question.strip()) < 3:
            raise ValueError("å•é¡Œç‚ºç©ºæˆ–éçŸ­")
        
        if self._contains_example_content(question):
            raise ValueError("å•é¡ŒåŒ…å«ç¯„ä¾‹å…§å®¹")
        
        # é©—è­‰ references æ ¼å¼
        references = result['references']
        if not isinstance(references, list):
            raise ValueError("references å¿…é ˆæ˜¯åˆ—è¡¨")
        
        if len(references) == 0:
            raise ValueError("references ä¸èƒ½ç‚ºç©º")
        
        if len(references) > 5:
            raise ValueError("references æ•¸é‡ä¸èƒ½è¶…é 5")
        
        # é©—è­‰æ¯å€‹ reference
        for i, ref in enumerate(references):
            if not isinstance(ref, dict):
                raise ValueError(f"references[{i}] å¿…é ˆæ˜¯å­—å…¸")
            
            required_ref_fields = ['content', 'start_chunk', 'end_chunk']
            for ref_field in required_ref_fields:
                if ref_field not in ref:
                    raise ValueError(f"references[{i}] ç¼ºå°‘å­—æ®µ: {ref_field}")
            
            # æª¢æŸ¥ content ä¸ç‚ºç©ºä¸”ä¸æ˜¯ç¯„ä¾‹
            if not ref['content'] or len(ref['content'].strip()) < 3:
                raise ValueError(f"references[{i}] content ç‚ºç©ºæˆ–éçŸ­")
            
            if self._contains_example_content(ref['content']):
                raise ValueError(f"references[{i}] content åŒ…å«ç¯„ä¾‹å…§å®¹")
        
        print(f"âœ… JSON é©—è­‰é€šé - å•é¡Œ: {question[:50]}...")
        print(f"   åƒè€ƒè³‡æ–™æ•¸é‡: {len(references)}")
        
        return result

    def _fix_common_json_issues(self, json_str: str) -> str:
        """
        ä¿®å¾©å¸¸è¦‹çš„ JSON æ ¼å¼å•é¡Œ
        """
        import re
        
        # ç§»é™¤å°¾éš¨é€—è™Ÿ
        fixed = re.sub(r',\s*}', '}', json_str)
        fixed = re.sub(r',\s*]', ']', fixed)
        
        # ä¿®å¾©ä¸­æ–‡æ¨™é»
        fixed = fixed.replace('ï¼Œ', ',').replace('ï¼š', ':')
        
        # ç§»é™¤å¯èƒ½çš„å¤šé¤˜å­—ç¬¦
        fixed = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', fixed)
        
        return fixed

    def _extract_json_fragments(self, text: str) -> dict:
        """
        æœ€å¾Œæ‰‹æ®µï¼šå¾æ–‡æœ¬ä¸­æå– JSON ç‰‡æ®µä¸¦å˜—è©¦é‡æ§‹
        """
        import re
        
        # å˜—è©¦æå–å•é¡Œ
        question_patterns = [
            r'"question":\s*"([^"]+)"',
            r"'question':\s*'([^']+)'",
            r'question[:\s]+"([^"]+)"'
        ]
        
        question = None
        for pattern in question_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match and not self._contains_example_content(match.group(1)):
                question = match.group(1)
                break
        
        if not question:
            raise ValueError("ç„¡æ³•æå–å•é¡Œ")
        
        # ç°¡å–®æ§‹é€ ä¸€å€‹åŸºæœ¬çš„å›æ‡‰
        return {
            "question": question,
            "references": [{"content": "ç„¡æ³•è§£æåƒè€ƒè³‡æ–™", "start_chunk": 0, "end_chunk": 0}]
        }

    def debug_prompt_cutting(self, corpus_id, use_approx=True):
        """
        èª¿è©¦ prompt åˆ‡å‰²æ•ˆæœçš„å‡½å¼
        """
        print("="*80)
        print("ğŸ” Prompt åˆ‡å‰²èª¿è©¦")
        print("="*80)
        
        # æº–å‚™æ•¸æ“š
        corpus = self._get_cleaned_document_content(corpus_id)
        if len(corpus) > 4000:
            document = corpus[:4000]
        else:
            document = corpus
        
        if use_approx:
            tagged_text, _ = self._tag_text(document)
            user_prompt = self.question_maker_approx_user_prompt.replace("{document}", tagged_text).replace("{prev_questions_str}", "")
            system_prompt = self.question_maker_approx_system_prompt
        else:
            user_prompt = self.question_maker_user_prompt.replace("{document}", document).replace("{prev_questions_str}", "")
            system_prompt = self.question_maker_system_prompt
        
        print("åŸ·è¡Œæ¨ç†ä¸¦åˆ‡å‰²...")
        response = self._run_llama_inference(system_prompt, user_prompt, 600)
        
        print("\n" + "="*80)
        print("ğŸ¯ åˆ‡å‰²çµæœæª¢æŸ¥")
        print("="*80)
        
        try:
            result = self._extract_json_from_response(response)
            print("âœ… æœ€çµ‚è§£ææˆåŠŸ!")
            print(f"å•é¡Œ: {result.get('question', 'NO QUESTION')}")
            print(f"åƒè€ƒè³‡æ–™æ•¸é‡: {len(result.get('references', []))}")
            
            for i, ref in enumerate(result.get('references', [])):
                print(f"  åƒè€ƒ {i+1}: {ref.get('content', '')[:50]}...")
            
            return result
            
        except Exception as e:
            print(f"âŒ æœ€çµ‚è§£æå¤±æ•—: {e}")
            return None

    def _extract_question_and_approx_references(self, corpus, document_length=4000, prev_questions=[]):
        if len(corpus) > document_length:
            start_index = random.randint(0, len(corpus) - document_length)
            document = corpus[start_index : start_index + document_length]
        else:
            start_index = 0
            document = corpus
        
        if prev_questions is not None:
            if len(prev_questions) > 20:
                questions_sample = random.sample(prev_questions, 20)
                prev_questions_str = '\n'.join(questions_sample)
            else:
                prev_questions_str = '\n'.join(prev_questions)
        else:
            prev_questions_str = ""

        tagged_text, tag_indexes = self._tag_text(document)

        user_prompt_filled = self.question_maker_approx_user_prompt.replace("{document}", tagged_text).replace("{prev_questions_str}", prev_questions_str)
        response_text = self._run_llama_inference(self.question_maker_approx_system_prompt, user_prompt_filled, 600)
        
        json_response = self._extract_json_from_response(response_text)
        
        try:
            text_references = json_response['references']
        except KeyError:
            raise ValueError("The response does not contain a 'references' field.")
        try:
            question = json_response['question']
        except KeyError:
            raise ValueError("The response does not contain a 'question' field.")

        references = []
        max_chunk_index = len(tag_indexes) - 2  # å¯ç”¨çš„æœ€å¤§chunkç´¢å¼• (å› ç‚ºæˆ‘å€‘è¦è¨ªå• index+1)
        
        print(f"ğŸ“Š Chunkç´¢å¼•èª¿è©¦è³‡è¨Š: tag_indexesé•·åº¦={len(tag_indexes)}, æœ€å¤§å¯ç”¨chunkç´¢å¼•={max_chunk_index}")
        
        for i, reference in enumerate(text_references):
            reference_keys = list(reference.keys())

            if len(reference_keys) != 3:
                raise ValueError(f"Each reference must have exactly 3 keys: 'content', 'start_chunk', and 'end_chunk'. Got keys: {reference_keys}")

            if 'start_chunk' not in reference_keys or 'end_chunk' not in reference_keys:
                raise ValueError("Each reference must contain 'start_chunk' and 'end_chunk' keys.")

            # æ·»åŠ ç´¢å¼•é‚Šç•Œæª¢æŸ¥
            start_chunk = reference['start_chunk']
            end_chunk = reference['end_chunk']
            
            print(f"  åƒè€ƒ {i+1}: start_chunk={start_chunk}, end_chunk={end_chunk}")
            
            # é©—è­‰ start_chunk ç´¢å¼•
            if start_chunk < 0 or start_chunk >= len(tag_indexes):
                print(f"âš ï¸  è­¦å‘Š: start_chunk {start_chunk} è¶…å‡ºç¯„åœ [0, {len(tag_indexes)-1}]ï¼Œèª¿æ•´ç‚ºæœ‰æ•ˆå€¼")
                start_chunk = max(0, min(start_chunk, len(tag_indexes)-1))
            
            # é©—è­‰ end_chunk ç´¢å¼• (éœ€è¦è€ƒæ…® +1 çš„è¨ªå•)
            if end_chunk < 0 or end_chunk >= max_chunk_index:
                print(f"âš ï¸  è­¦å‘Š: end_chunk {end_chunk} è¶…å‡ºå®‰å…¨ç¯„åœ [0, {max_chunk_index}]ï¼Œèª¿æ•´ç‚ºæœ‰æ•ˆå€¼")
                end_chunk = max(0, min(end_chunk, max_chunk_index))
            
            # ç¢ºä¿ start_chunk <= end_chunk
            if start_chunk > end_chunk:
                print(f"âš ï¸  è­¦å‘Š: start_chunk ({start_chunk}) > end_chunk ({end_chunk})ï¼Œäº¤æ›æ•¸å€¼")
                start_chunk, end_chunk = end_chunk, start_chunk

            # ä½¿ç”¨ä¿®æ­£å¾Œçš„ç´¢å¼•é€²è¡Œè¨ˆç®—
            try:
                if 'end_chunk' not in reference_keys:
                    reference_keys.remove('content')
                    reference_keys.remove('start_chunk')
                    end_chunk_key = reference_keys[0]
                    end_index = start_index + tag_indexes[reference[end_chunk_key]+1]
                else:
                    # å®‰å…¨åœ°è¨ªå• tag_indexes
                    if end_chunk + 1 < len(tag_indexes):
                        end_index = start_index + tag_indexes[end_chunk + 1]
                    else:
                        # å¦‚æœ end_chunk+1 è¶…å‡ºç¯„åœï¼Œä½¿ç”¨æœ€å¾Œä¸€å€‹å¯ç”¨çš„ç´¢å¼•
                        print(f"âš ï¸  end_chunk+1 ({end_chunk+1}) è¶…å‡º tag_indexes ç¯„åœï¼Œä½¿ç”¨æœ€å¾Œå¯ç”¨ä½ç½®")
                        end_index = start_index + tag_indexes[-1]

                start_index_ref = start_index + tag_indexes[start_chunk]
                
                # é¡å¤–çš„é‚Šç•Œæª¢æŸ¥ï¼šç¢ºä¿ä¸è¶…å‡ºåŸå§‹corpusç¯„åœ
                end_index = min(end_index, len(corpus))
                start_index_ref = min(start_index_ref, len(corpus))
                
                # ç¢ºä¿ start_index_ref <= end_index
                if start_index_ref > end_index:
                    print(f"âš ï¸  è­¦å‘Š: start_index_ref ({start_index_ref}) > end_index ({end_index})ï¼Œèª¿æ•´ç‚ºç›¸åŒå€¼")
                    end_index = start_index_ref
                
                print(f"    æœ€çµ‚ç´¢å¼•: start_index_ref={start_index_ref}, end_index={end_index}")
                
                extracted_text = corpus[start_index_ref:end_index]
                if len(extracted_text.strip()) == 0:
                    print(f"âš ï¸  è­¦å‘Š: æå–çš„æ–‡æœ¬ç‚ºç©ºï¼Œè·³éæ­¤åƒè€ƒ")
                    continue
                    
                references.append((extracted_text, start_index_ref, end_index))
                
            except IndexError as e:
                print(f"âŒ ç´¢å¼•éŒ¯èª¤ (åƒè€ƒ {i+1}): {e}")
                print(f"   start_chunk={start_chunk}, end_chunk={end_chunk}")
                print(f"   tag_indexesé•·åº¦={len(tag_indexes)}")
                print(f"   å˜—è©¦è¨ªå•çš„ç´¢å¼•: {end_chunk+1}")
                # è·³éé€™å€‹æœ‰å•é¡Œçš„åƒè€ƒï¼Œè€Œä¸æ˜¯è®“æ•´å€‹ç¨‹åºå´©æ½°
                continue
            except Exception as e:
                print(f"âŒ å…¶ä»–éŒ¯èª¤ (åƒè€ƒ {i+1}): {e}")
                continue
        
        # æª¢æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€å€‹æœ‰æ•ˆçš„åƒè€ƒ
        if len(references) == 0:
            raise ValueError("No valid references could be extracted from the model response.")
        
        print(f"âœ… æˆåŠŸæå– {len(references)} å€‹åƒè€ƒè³‡æ–™")
        return question, references

    def _extract_question_and_references(self, corpus, document_length=4000, prev_questions=[]):
        if len(corpus) > document_length:
            start_index = random.randint(0, len(corpus) - document_length)
            document = corpus[start_index : start_index + document_length]
        else:
            document = corpus
        
        if prev_questions is not None:
            if len(prev_questions) > 20:
                questions_sample = random.sample(prev_questions, 20)
                prev_questions_str = '\n'.join(questions_sample)
            else:
                prev_questions_str = '\n'.join(prev_questions)
        else:
            prev_questions_str = ""

        user_prompt_filled = self.question_maker_user_prompt.replace("{document}", document).replace("{prev_questions_str}", prev_questions_str)
        response_text = self._run_llama_inference(self.question_maker_system_prompt, user_prompt_filled, 600)

        json_response = self._extract_json_from_response(response_text)
        
        try:
            text_references = json_response['references']
        except KeyError:
            raise ValueError("The response does not contain a 'references' field.")
        try:
            question = json_response['question']
        except KeyError:
            raise ValueError("The response does not contain a 'question' field.")

        references = []
        for reference in text_references:
            if not isinstance(reference, str):
                raise ValueError(f"Expected reference to be of type str, but got {type(reference).__name__}")
            target = rigorous_document_search(corpus, reference)
            if target is not None:
                reference, start_index, end_index = target
                references.append((reference, start_index, end_index))
            else:
                raise ValueError(f"No match found in the document for the given reference.\nReference: {reference}")
        
        return question, references

    def _generate_corpus_questions(self, corpus_id, approx=False, n=5):
        corpus = self._get_cleaned_document_content(corpus_id)
        
        if not corpus:
            print(f"æ–‡ä»¶ {corpus_id} å…§å®¹ç‚ºç©ºæˆ–ç„¡æ³•è™•ç†ï¼Œè·³éã€‚")
            return

        i = 0
        while i < n:
            retry_count = 0
            max_retries = 3
            success = False
            
            while retry_count < max_retries and not success:
                try:
                    print(f"Trying Query {i} for {corpus_id} (attempt {retry_count + 1})")
                    questions_list = self.synth_questions_df[self.synth_questions_df['corpus_id'] == corpus_id]['question'].tolist()
                    
                    if approx:
                        question, references = self._extract_question_and_approx_references(corpus, 4000, questions_list)
                    else:
                        question, references = self._extract_question_and_references(corpus, 4000, questions_list)
                    
                    if len(references) > 5:
                        raise ValueError("The number of references exceeds 5.")
                    
                    # é©—è­‰å•é¡Œæœ‰æ•ˆæ€§
                    if not question or len(question.strip()) < 3:
                        raise ValueError("Generated question is too short or empty")
                    
                    # é¡å¤–é©—è­‰ï¼šç¢ºä¿å•é¡Œä¸åŒ…å«ç¯„ä¾‹å…§å®¹
                    if self._contains_example_content(question):
                        raise ValueError("Generated question contains example content")
                    
                    print(f"SUCCESS: Generated question: {question}")
                    
                    # ä¿®æ­£ï¼šç›´æ¥ä½¿ç”¨ list of dictï¼Œä¸é€²è¡Œé¡å¤–çš„ JSON åºåˆ—åŒ–
                    references_list = [{'content': ref[0], 'start_index': ref[1], 'end_index': ref[2]} for ref in references]
                    new_question = {
                        'question': question,
                        'references': references_list,  # ç›´æ¥å„²å­˜ç‚º listï¼Œç¨å¾Œçµ±ä¸€åºåˆ—åŒ–
                        'corpus_id': corpus_id
                    }

                    new_df = pd.DataFrame([new_question])
                    self.synth_questions_df = pd.concat([self.synth_questions_df, new_df], ignore_index=True)
                    self._save_questions_df()
                    
                    success = True
                    print(f"Question {i} saved successfully")

                except (ValueError, json.JSONDecodeError) as e:
                    print(f"Error occurred (attempt {retry_count + 1}): {e}")
                    retry_count += 1
                    
                    if retry_count >= max_retries:
                        print(f"Max retries reached for query {i}, skipping...")
                        break
            
            if success:
                i += 1
            else:
                print(f"Failed to generate question {i}, moving to next")
                i += 1

    def _get_synth_questions_df(self):
        """
        ä¿®æ­£çš„è¼‰å…¥å‡½æ•¸ï¼Œè™•ç†å¯èƒ½çš„é›™é‡åºåˆ—åŒ–å•é¡Œ
        """
        if os.path.exists(self.questions_csv_path):
            synth_questions_df = pd.read_csv(self.questions_csv_path)
            
            # ä¿®æ­£ references æ¬„ä½çš„è§£æå•é¡Œ
            if 'references' in synth_questions_df.columns:
                def fix_references(refs_str):
                    try:
                        # ä½¿ç”¨å®‰å…¨çš„ JSON è§£æ
                        parsed_refs = self._safe_json_loads(refs_str)
                        return parsed_refs
                    except:
                        print(f"ç„¡æ³•è§£æ references: {refs_str[:100]}...")
                        return []
                
                synth_questions_df['references'] = synth_questions_df['references'].apply(fix_references)
        else:
            synth_questions_df = pd.DataFrame(columns=['question', 'references', 'corpus_id'])
        return synth_questions_df

    def generate_queries_and_excerpts(self, approximate_excerpts=False, num_rounds = -1, queries_per_corpus = 5):
        self.synth_questions_df = self._get_synth_questions_df()

        rounds = 0
        while num_rounds == -1 or rounds < num_rounds:
            for corpus_id in self.corpora_paths:
                self._generate_corpus_questions(corpus_id, approx=approximate_excerpts, n=queries_per_corpus)
            rounds += 1

    def _get_sim(self, target, references):
        texts = [target] + references
        embedding_function = get_bge_m3_embedding_function()
        
        # ä¿®æ”¹é€™è£¡ï¼šä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•èª¿ç”¨
        # é¸é …1ï¼šå¦‚æœ HuggingFaceBgeEmbeddings æœ‰ embed_documents æ–¹æ³•
        try:
            embeddings = embedding_function.embed_documents(texts)
        except AttributeError:
            # é¸é …2ï¼šå¦‚æœåªæœ‰ embed_query æ–¹æ³•ï¼Œé€å€‹è™•ç†
            embeddings = [embedding_function.embed_query(text) for text in texts]
        
        nparray1 = embeddings[0]

        full_sim = []
        for i in range(1, len(embeddings)):
            nparray2 = embeddings[i]
            cosine_similarity = np.dot(nparray1, nparray2) / (np.linalg.norm(nparray1) * np.linalg.norm(nparray2))
            full_sim.append(cosine_similarity)

        return full_sim

    def _corpus_filter_poor_highlights(self, corpus_id, synth_questions_df, threshold):
        corpus_questions_df = synth_questions_df[synth_questions_df['corpus_id'] == corpus_id].copy()

        def edit_row(row):
            question = row['question']
            
            # ä¿®æ­£ï¼šä½¿ç”¨å®‰å…¨çš„ JSON è§£æ
            try:
                references_data = row['references']
                if isinstance(references_data, str):
                    # å¦‚æœæ˜¯å­—ä¸²ï¼Œè§£æå®ƒ
                    references_list = self._safe_json_loads(references_data)
                elif isinstance(references_data, list):
                    # å¦‚æœå·²ç¶“æ˜¯ listï¼Œç›´æ¥ä½¿ç”¨
                    references_list = references_data
                else:
                    print(f"æœªçŸ¥çš„ references æ ¼å¼: {type(references_data)}")
                    references_list = []
                
                references = [ref['content'] for ref in references_list if isinstance(ref, dict) and 'content' in ref]
                
                if references:
                    similarity_scores = self._get_sim(question, references)
                    if similarity_scores:
                        worst_ref_score = min(similarity_scores)
                        row['worst_ref_score'] = worst_ref_score
                    else:
                        row['worst_ref_score'] = -1.0
                else:
                    row['worst_ref_score'] = -1.0
                    
            except Exception as e:
                print(f"è™•ç† row æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                row['worst_ref_score'] = -1.0
            
            return row

        corpus_questions_df = corpus_questions_df.apply(edit_row, axis=1)

        count_before = len(corpus_questions_df)

        corpus_questions_df = corpus_questions_df[corpus_questions_df['worst_ref_score'] >= threshold]
        corpus_questions_df = corpus_questions_df.drop(columns=['worst_ref_score'])

        count_after = len(corpus_questions_df)

        print(f"Corpus: {corpus_id} - Removed {count_before - count_after} .")

        # ç¢ºä¿ references æ­£ç¢ºåºåˆ—åŒ–
        def ensure_json_string(refs):
            if isinstance(refs, list):
                return json.dumps(refs, ensure_ascii=False)
            elif isinstance(refs, str):
                # é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆ JSON
                try:
                    json.loads(refs)
                    return refs
                except:
                    return json.dumps([], ensure_ascii=False)
            else:
                return json.dumps([], ensure_ascii=False)
        
        corpus_questions_df['references'] = corpus_questions_df['references'].apply(ensure_json_string)

        full_questions_df = pd.read_csv(self.questions_csv_path)
        full_questions_df = full_questions_df[full_questions_df['corpus_id'] != corpus_id]

        full_questions_df = pd.concat([full_questions_df, corpus_questions_df], ignore_index=True)
        for col in ['fixed', 'worst_ref_score', 'diff_score']:
            if col in full_questions_df.columns:
                full_questions_df = full_questions_df.drop(columns=col)

        full_questions_df.to_csv(self.questions_csv_path, index=False)

    def filter_poor_excerpts(self, threshold=0.36, corpora_subset=[]):
        if os.path.exists(self.questions_csv_path):
            synth_questions_df = pd.read_csv(self.questions_csv_path)
            if len(synth_questions_df) > 0:
                corpus_list = synth_questions_df['corpus_id'].unique().tolist()
                if corpora_subset:
                    corpus_list = [c for c in corpus_list if c in corpora_subset]
                for corpus_id in corpus_list:
                    self._corpus_filter_poor_highlights(corpus_id, synth_questions_df, threshold)

    def _corpus_filter_duplicates(self, corpus_id, synth_questions_df, threshold):
        corpus_questions_df = synth_questions_df[synth_questions_df['corpus_id'] == corpus_id].copy()
        count_before = len(corpus_questions_df)
        
        # å…ˆå»é™¤å®Œå…¨ç›¸åŒçš„å•é¡Œ
        corpus_questions_df.drop_duplicates(subset='question', keep='first', inplace=True)
        
        questions = corpus_questions_df['question'].tolist()
        
        # å¦‚æœåªæœ‰ä¸€å€‹æˆ–é›¶å€‹å•é¡Œï¼Œç›´æ¥è¿”å›
        if len(questions) <= 1:
            print(f"Corpus: {corpus_id} - Only {len(questions)} question(s), no duplicates to filter.")
            # ä¿å­˜è™•ç†å¾Œçš„çµæœ - ä¿®æ­£åºåˆ—åŒ–å•é¡Œ
            def ensure_json_string(refs):
                if isinstance(refs, list):
                    return json.dumps(refs, ensure_ascii=False)
                elif isinstance(refs, str):
                    try:
                        json.loads(refs)
                        return refs
                    except:
                        return json.dumps([], ensure_ascii=False)
                else:
                    return json.dumps([], ensure_ascii=False)
            
            corpus_questions_df['references'] = corpus_questions_df['references'].apply(ensure_json_string)
            full_questions_df = pd.read_csv(self.questions_csv_path)
            full_questions_df = full_questions_df[full_questions_df['corpus_id'] != corpus_id]
            full_questions_df = pd.concat([full_questions_df, corpus_questions_df], ignore_index=True)
            for col in ['fixed', 'worst_ref_score', 'diff_score']:
                if col in full_questions_df.columns:
                    full_questions_df = full_questions_df.drop(columns=col)
            full_questions_df.to_csv(self.questions_csv_path, index=False)
            return
        
        # ä½¿ç”¨æ­£ç¢ºçš„ embedding å‡½å¼
        try:
            # æ–¹æ³•1: å˜—è©¦ä½¿ç”¨ BGE-M3 embedding function
            embedding_function = get_bge_m3_embedding_function()
            embeddings_list = []
            print(f"æ­£åœ¨è¨ˆç®— {len(questions)} å€‹å•é¡Œçš„ embeddings...")
            for i, question in enumerate(questions):
                if i % 10 == 0:  # æ¯10å€‹æ‰“å°ä¸€æ¬¡é€²åº¦
                    print(f"  è™•ç†é€²åº¦: {i+1}/{len(questions)}")
                try:
                    # å˜—è©¦ embed_documents æ–¹æ³•
                    embedding = embedding_function.embed_documents([question])[0]
                except AttributeError:
                    # å¦‚æœæ²’æœ‰ embed_documentsï¼Œä½¿ç”¨ embed_query
                    embedding = embedding_function.embed_query(question)
                embeddings_list.append(embedding)
            
            embeddings_matrix = np.array(embeddings_list)
            
        except Exception as e:
            print(f"BGE-M3 embedding å¤±æ•—: {e}")
            print("å˜—è©¦ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ...")
            
            # æ–¹æ³•2: å‚™ç”¨æ–¹æ¡ˆ - ä½¿ç”¨ç°¡å–®çš„æ–‡æœ¬ç›¸ä¼¼åº¦
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                from sklearn.metrics.pairwise import cosine_similarity
                
                print("ä½¿ç”¨ TF-IDF + Cosine Similarity ä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ")
                vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
                tfidf_matrix = vectorizer.fit_transform(questions)
                dot_product_matrix = cosine_similarity(tfidf_matrix)
                
            except ImportError:
                print("sklearn æœªå®‰è£ï¼Œä½¿ç”¨åŸºæ–¼å­—ç¬¦çš„ç°¡å–®ç›¸ä¼¼åº¦")
                # æ–¹æ³•3: æœ€å¾Œæ‰‹æ®µ - åŸºæ–¼å­—ç¬¦çš„ç›¸ä¼¼åº¦
                def simple_similarity(s1, s2):
                    """è¨ˆç®—å…©å€‹å­—ç¬¦ä¸²çš„ç°¡å–®ç›¸ä¼¼åº¦"""
                    s1_set = set(s1.lower().replace(' ', ''))
                    s2_set = set(s2.lower().replace(' ', ''))
                    if len(s1_set) == 0 and len(s2_set) == 0:
                        return 1.0
                    if len(s1_set) == 0 or len(s2_set) == 0:
                        return 0.0
                    intersection = len(s1_set.intersection(s2_set))
                    union = len(s1_set.union(s2_set))
                    return intersection / union
                
                # æ§‹å»ºç›¸ä¼¼åº¦çŸ©é™£
                n = len(questions)
                dot_product_matrix = np.zeros((n, n))
                print("è¨ˆç®—å­—ç¬¦ç›¸ä¼¼åº¦çŸ©é™£...")
                for i in range(n):
                    for j in range(i, n):
                        if i == j:
                            dot_product_matrix[i][j] = 1.0
                        else:
                            sim = simple_similarity(questions[i], questions[j])
                            dot_product_matrix[i][j] = sim
                            dot_product_matrix[j][i] = sim
                
                # è·³éå¾ŒçºŒçš„ embedding è™•ç†
                similarity_pairs = [(i, j, dot_product_matrix[i][j]) for i in range(len(dot_product_matrix)) for j in range(i+1, len(dot_product_matrix))]
                similarity_pairs.sort(key=lambda x: x[2], reverse=True)
                similarity_scores = np.array([x[2] for x in similarity_pairs])
                most_similars = (dot_product_matrix - np.eye(dot_product_matrix.shape[0])).max(axis=1)
                
                # åŸ·è¡Œéæ¿¾
                def filter_vectors(sim_matrix, threshold):
                    n = sim_matrix.shape[0]
                    remaining = np.ones(n, dtype=bool)
                    for i in range(n):
                        if remaining[i] == 1:
                            for j in range(i+1, n):
                                if remaining[j] == 1 and sim_matrix[i, j] > threshold:
                                    remaining[j] = 0
                    return remaining
                
                rows_to_keep = filter_vectors(dot_product_matrix, threshold)
                corpus_questions_df = corpus_questions_df.iloc[rows_to_keep]
                count_after = len(corpus_questions_df)
                print(f"Corpus: {corpus_id} - Removed {count_before - count_after} questions due to similarity.")
                
                # ä¿å­˜çµæœ - ä¿®æ­£åºåˆ—åŒ–å•é¡Œ
                def ensure_json_string(refs):
                    if isinstance(refs, list):
                        return json.dumps(refs, ensure_ascii=False)
                    elif isinstance(refs, str):
                        try:
                            json.loads(refs)
                            return refs
                        except:
                            return json.dumps([], ensure_ascii=False)
                    else:
                        return json.dumps([], ensure_ascii=False)
                
                corpus_questions_df['references'] = corpus_questions_df['references'].apply(ensure_json_string)
                full_questions_df = pd.read_csv(self.questions_csv_path)
                full_questions_df = full_questions_df[full_questions_df['corpus_id'] != corpus_id]
                full_questions_df = pd.concat([full_questions_df, corpus_questions_df], ignore_index=True)
                for col in ['fixed', 'worst_ref_score', 'diff_score']:
                    if col in full_questions_df.columns:
                        full_questions_df = full_questions_df.drop(columns=col)
                full_questions_df.to_csv(self.questions_csv_path, index=False)
                return
        
        # å¦‚æœ BGE-M3 æˆåŠŸï¼Œç¹¼çºŒæ­£å¸¸çš„ embedding è™•ç†
        print("è¨ˆç®— cosine similarity çŸ©é™£...")
        # è¨ˆç®—é»ç©çŸ©é™£ (å‡è¨­ embeddings å·²ç¶“æ¨™æº–åŒ–)
        dot_product_matrix = np.dot(embeddings_matrix, embeddings_matrix.T)
        
        # å¦‚æœ embeddings æ²’æœ‰æ¨™æº–åŒ–ï¼Œä½¿ç”¨ cosine similarity
        # from sklearn.metrics.pairwise import cosine_similarity
        # dot_product_matrix = cosine_similarity(embeddings_matrix)
        
        similarity_pairs = [(i, j, dot_product_matrix[i][j]) for i in range(len(dot_product_matrix)) for j in range(i+1, len(dot_product_matrix))]
        similarity_pairs.sort(key=lambda x: x[2], reverse=True)
        similarity_scores = np.array([x[2] for x in similarity_pairs])
        most_similars = (dot_product_matrix - np.eye(dot_product_matrix.shape[0])).max(axis=1)
        
        def filter_vectors(sim_matrix, threshold):
            n = sim_matrix.shape[0]
            remaining = np.ones(n, dtype=bool)
            for i in range(n):
                if remaining[i] == 1:
                    for j in range(i+1, n):
                        if remaining[j] == 1 and sim_matrix[i, j] > threshold:
                            remaining[j] = 0
            return remaining
        
        rows_to_keep = filter_vectors(dot_product_matrix, threshold)
        corpus_questions_df = corpus_questions_df.iloc[rows_to_keep]
        count_after = len(corpus_questions_df)
        
        print(f"Corpus: {corpus_id} - Removed {count_before - count_after} questions due to similarity.")
        
        # ä¿å­˜çµæœ - ä¿®æ­£åºåˆ—åŒ–å•é¡Œ
        def ensure_json_string(refs):
            if isinstance(refs, list):
                return json.dumps(refs, ensure_ascii=False)
            elif isinstance(refs, str):
                try:
                    json.loads(refs)
                    return refs
                except:
                    return json.dumps([], ensure_ascii=False)
            else:
                return json.dumps([], ensure_ascii=False)
        
        corpus_questions_df['references'] = corpus_questions_df['references'].apply(ensure_json_string)
        full_questions_df = pd.read_csv(self.questions_csv_path)
        full_questions_df = full_questions_df[full_questions_df['corpus_id'] != corpus_id]
        full_questions_df = pd.concat([full_questions_df, corpus_questions_df], ignore_index=True)
        for col in ['fixed', 'worst_ref_score', 'diff_score']:
            if col in full_questions_df.columns:
                full_questions_df = full_questions_df.drop(columns=col)
        full_questions_df.to_csv(self.questions_csv_path, index=False)

    def filter_duplicates(self, threshold=0.78, corpora_subset=[]):
        if os.path.exists(self.questions_csv_path):
            synth_questions_df = pd.read_csv(self.questions_csv_path)
            if len(synth_questions_df) > 0:
                corpus_list = synth_questions_df['corpus_id'].unique().tolist()
                if corpora_subset:
                    corpus_list = [c for c in corpus_list if c in corpora_subset]
                for corpus_id in corpus_list:
                    self._corpus_filter_duplicates(corpus_id, synth_questions_df, threshold)

    def question_ref_filter(self):
        self.synth_questions_df = self._get_synth_questions_df()

    def debug_full_output(self, corpus_id, use_approx=True, save_to_file=False):
        """
        å¢å¼·ç‰ˆèª¿è©¦å‡½å¼ï¼šæª¢æŸ¥æ¨¡å‹çš„å®Œæ•´è¼¸å…¥å’Œè¼¸å‡ºï¼ŒåŒ…å«æ”¹é€²çš„è§£æé‚è¼¯
        
        Args:
            corpus_id: æ–‡æª”è·¯å¾‘
            use_approx: æ˜¯å¦ä½¿ç”¨ approximate æ¨¡å¼
            save_to_file: æ˜¯å¦å°‡çµæœä¿å­˜åˆ°æ–‡ä»¶
        """
        print("="*100)
        print("ğŸ” å®Œæ•´è¼¸å…¥è¼¸å‡ºèª¿è©¦ (å¢å¼·ç‰ˆ)")
        print("="*100)
        
        debug_results = {
            'corpus_id': corpus_id,
            'use_approx': use_approx,
            'steps': {}
        }
        
        # 1. æª¢æŸ¥æ–‡æª”æ¸…ç†å¾Œçš„å…§å®¹
        print("ğŸ“„ æ­¥é©Ÿ 1: æª¢æŸ¥æ–‡æª”æ¸…ç†çµæœ")
        print("-" * 50)
        corpus = self._get_cleaned_document_content(corpus_id)
        if not corpus:
            print("âŒ ERROR: ç„¡æ³•è®€å–æˆ–æ¸…ç†æ–‡æª”å…§å®¹")
            return None
        
        debug_results['steps']['document_cleaning'] = {
            'success': True,
            'total_length': len(corpus),
            'preview': corpus[:500]
        }
        
        print(f"æ–‡æª”ç¸½é•·åº¦: {len(corpus)} å­—ç¬¦")
        print("å‰ 500 å­—ç¬¦:")
        print(repr(corpus[:500]))
        print("\nå¾Œ 500 å­—ç¬¦:")
        print(repr(corpus[-500:]))
        print()
        
        # 2. æª¢æŸ¥æ–‡æª”ç‰‡æ®µé¸æ“‡
        print("ğŸ“‹ æ­¥é©Ÿ 2: æª¢æŸ¥é¸å–çš„æ–‡æª”ç‰‡æ®µ")
        print("-" * 50)
        if len(corpus) > 4000:
            start_index = 0  # å›ºå®šé¸æ“‡é–‹é ­ï¼Œä¾¿æ–¼èª¿è©¦
            document = corpus[start_index:start_index + 4000]
            print(f"é¸å–ç‰‡æ®µ: ä½ç½® {start_index} åˆ° {start_index + 4000}")
        else:
            document = corpus
            print("æ–‡æª”é•·åº¦å°æ–¼ 4000ï¼Œä½¿ç”¨å®Œæ•´å…§å®¹")
        
        debug_results['steps']['document_selection'] = {
            'selected_length': len(document),
            'selection_start': 0,
            'selection_end': len(document)
        }
        
        print(f"é¸å–ç‰‡æ®µé•·åº¦: {len(document)} å­—ç¬¦")
        print("é¸å–ç‰‡æ®µå‰ 300 å­—ç¬¦:")
        print(repr(document[:300]))
        print()
        
        # 3. æª¢æŸ¥ prompt çµ„è£
        print("ğŸ› ï¸ æ­¥é©Ÿ 3: æª¢æŸ¥ Prompt çµ„è£")
        print("-" * 50)
        
        if use_approx:
            print("ä½¿ç”¨ APPROXIMATE æ¨¡å¼")
            tagged_text, tag_indexes = self._tag_text(document)
            print(f"æ¨™è¨˜å¾Œæ–‡æœ¬é•·åº¦: {len(tagged_text)} å­—ç¬¦")
            print("æ¨™è¨˜å¾Œæ–‡æœ¬å‰ 500 å­—ç¬¦:")
            print(repr(tagged_text[:500]))
            
            user_prompt = self.question_maker_approx_user_prompt.replace("{document}", tagged_text).replace("{prev_questions_str}", "")
            system_prompt = self.question_maker_approx_system_prompt
        else:
            print("ä½¿ç”¨ EXACT æ¨¡å¼")
            user_prompt = self.question_maker_user_prompt.replace("{document}", document).replace("{prev_questions_str}", "")
            system_prompt = self.question_maker_system_prompt
        
        debug_results['steps']['prompt_assembly'] = {
            'system_prompt_length': len(system_prompt),
            'user_prompt_length': len(user_prompt),
            'mode': 'APPROXIMATE' if use_approx else 'EXACT'
        }
        
        print(f"System prompt é•·åº¦: {len(system_prompt)} å­—ç¬¦")
        print("System prompt å‰ 300 å­—ç¬¦:")
        print(repr(system_prompt[:300]))
        print()
        
        print(f"User prompt é•·åº¦: {len(user_prompt)} å­—ç¬¦")
        print("User prompt å‰ 500 å­—ç¬¦:")
        print(repr(user_prompt[:500]))
        print("User prompt å¾Œ 500 å­—ç¬¦:")
        print(repr(user_prompt[-500:]))
        print()
        
        # 4. æª¢æŸ¥å®Œæ•´çš„æ¨¡å‹è¼¸å…¥
        print("ğŸ“¤ æ­¥é©Ÿ 4: æª¢æŸ¥ç™¼é€çµ¦æ¨¡å‹çš„å®Œæ•´è¼¸å…¥")
        print("-" * 50)
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        full_prompt = self.llama_tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        json_instruction = "\n\nIMPORTANT: You must respond with ONLY a valid JSON object. Do not include any explanations, comments, or additional text outside the JSON structure."
        full_prompt += json_instruction
        
        debug_results['steps']['model_input'] = {
            'full_prompt_length': len(full_prompt),
            'has_json_instruction': 'IMPORTANT' in full_prompt
        }
        
        print(f"å®Œæ•´ prompt é•·åº¦: {len(full_prompt)} å­—ç¬¦")
        print("å®Œæ•´ prompt å‰ 800 å­—ç¬¦:")
        print(repr(full_prompt[:800]))
        print("\nå®Œæ•´ prompt å¾Œ 800 å­—ç¬¦:")
        print(repr(full_prompt[-800:]))
        print()
        
        # 5. ç²å–æ¨¡å‹åŸå§‹è¼¸å‡º
        print("ğŸ“¥ æ­¥é©Ÿ 5: æ¨¡å‹åŸå§‹è¼¸å‡º")
        print("-" * 50)
        print("æ­£åœ¨ç”Ÿæˆå›æ‡‰...")
        
        inputs = self.llama_tokenizer(
            full_prompt, 
            return_tensors="pt", 
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.llama_model.device)
        
        with torch.no_grad():
            outputs = self.llama_model.generate(
                **inputs,
                max_new_tokens=600,
                do_sample=True,
                temperature=0.1,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.05,
                pad_token_id=self.llama_tokenizer.eos_token_id,
                eos_token_id=self.llama_tokenizer.eos_token_id,
                use_cache=True
            )
        
        # è§£ç¢¼å®Œæ•´è¼¸å‡º
        full_response = self.llama_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print("ğŸ¤– æ¨¡å‹å®Œæ•´åŸå§‹è¼¸å‡º:")
        print("="*60)
        print(full_response)
        print("="*60)
        print(f"åŸå§‹è¼¸å‡ºé•·åº¦: {len(full_response)} å­—ç¬¦")
        print()
        
        debug_results['steps']['model_output'] = {
            'full_response_length': len(full_response),
            'full_response': full_response
        }
        
        # 6. ä½¿ç”¨æ”¹é€²çš„å›æ‡‰æå–é‚è¼¯
        print("ğŸ”§ æ­¥é©Ÿ 6: æ”¹é€²çš„å›æ‡‰æå–")
        print("-" * 50)
        
        extracted_response = self._extract_model_response_from_full_output(full_response, full_prompt)
        
        print("æå–çš„å›æ‡‰:")
        print("="*60)
        print(extracted_response)
        print("="*60)
        print(f"æå–çš„å›æ‡‰é•·åº¦: {len(extracted_response)} å­—ç¬¦")
        
        debug_results['steps']['response_extraction'] = {
            'extracted_length': len(extracted_response),
            'contains_example': self._contains_example_content(extracted_response)
        }
        
        if self._contains_example_content(extracted_response):
            print("âš ï¸ è­¦å‘Šï¼šæå–çš„å›æ‡‰åŒ…å«ç¯„ä¾‹å…§å®¹!")
        print()
        
        # 7. ä½¿ç”¨æ”¹é€²çš„ JSON è§£æé‚è¼¯
        print("ğŸ§ª æ­¥é©Ÿ 7: æ”¹é€²çš„ JSON è§£æ")
        print("-" * 50)
        
        try:
            parsed_result = self._extract_json_from_response(extracted_response)
            print("âœ… è§£ææˆåŠŸ!")
            print("è§£æçµæœ:")
            print(f"- å•é¡Œ: {parsed_result.get('question', 'NO QUESTION')}")
            print(f"- åƒè€ƒè³‡æ–™æ•¸é‡: {len(parsed_result.get('references', []))}")
            
            debug_results['steps']['json_parsing'] = {
                'success': True,
                'question': parsed_result.get('question', ''),
                'references_count': len(parsed_result.get('references', []))
            }
            
            if 'references' in parsed_result:
                for i, ref in enumerate(parsed_result['references']):
                    print(f"  åƒè€ƒ {i+1}: {ref.get('content', 'NO CONTENT')[:100]}...")
                    if 'start_chunk' in ref:
                        print(f"    chunks: {ref['start_chunk']} - {ref['end_chunk']}")
            
        except Exception as e:
            print(f"âŒ è§£æå¤±æ•—: {e}")
            debug_results['steps']['json_parsing'] = {
                'success': False,
                'error': str(e)
            }
            parsed_result = None
        
        # 8. å¯é¸ï¼šä¿å­˜åˆ°æ–‡ä»¶
        if save_to_file:
            debug_filename = f"debug_output_{corpus_id.replace('/', '_').replace('\\', '_')}.txt"
            with open(debug_filename, 'w', encoding='utf-8') as f:
                f.write("="*100 + "\n")
                f.write("å®Œæ•´è¼¸å…¥è¼¸å‡ºèª¿è©¦å ±å‘Š (å¢å¼·ç‰ˆ)\n")
                f.write("="*100 + "\n\n")
                f.write(f"æ–‡æª”è·¯å¾‘: {corpus_id}\n")
                f.write(f"æ¨¡å¼: {'APPROXIMATE' if use_approx else 'EXACT'}\n\n")
                f.write("èª¿è©¦çµæœæ‘˜è¦:\n")
                f.write(json.dumps(debug_results, indent=2, ensure_ascii=False) + "\n\n")
                f.write("å®Œæ•´ Prompt:\n")
                f.write(full_prompt + "\n\n")
                f.write("æ¨¡å‹åŸå§‹è¼¸å‡º:\n")
                f.write(full_response + "\n\n")
                f.write("æå–çš„å›æ‡‰:\n")
                f.write(extracted_response + "\n")
            
            print(f"ğŸ“ èª¿è©¦çµæœå·²ä¿å­˜åˆ°: {debug_filename}")
        
        print("\n" + "="*100)
        print("ğŸ¯ èª¿è©¦å®Œæˆ")
        print("="*100)
        
        return {
            'document_content': corpus,
            'selected_fragment': document,
            'full_prompt': full_prompt,
            'model_full_response': full_response,
            'extracted_response': extracted_response,
            'parsed_result': parsed_result,
            'debug_results': debug_results
        }

    def repair_csv_references(self):
        """
        ä¿®å¾© CSV ä¸­çš„é›™é‡åºåˆ—åŒ–å•é¡Œ
        """
        print("ğŸ”§ é–‹å§‹ä¿®å¾© CSV ä¸­çš„ references æ¬„ä½...")
        
        if not os.path.exists(self.questions_csv_path):
            print("âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        # å‰µå»ºå‚™ä»½
        backup_path = self.questions_csv_path.replace('.csv', '_backup.csv')
        if not os.path.exists(backup_path):
            import shutil
            shutil.copy2(self.questions_csv_path, backup_path)
            print(f"âœ… å·²å‰µå»ºå‚™ä»½: {backup_path}")
        
        # è®€å–ä¸¦ä¿®å¾©
        df = pd.read_csv(self.questions_csv_path)
        if 'corpus_id' in df.columns:
            df['corpus_id'] = df['corpus_id'].str.replace('\\', '/', regex=False)
            print("ğŸ”§ å·²ä¿®æ­£ 'corpus_id' æ¬„ä½ä¸­çš„è·¯å¾‘æ ¼å¼ã€‚")
        print(f"ğŸ“Š è®€å–åˆ° {len(df)} æ¢è¨˜éŒ„")
        
        fixed_count = 0
        error_count = 0
        
        def fix_reference_field(refs_str):
            nonlocal fixed_count, error_count
            
            if pd.isna(refs_str) or refs_str == '':
                return json.dumps([], ensure_ascii=False)
            
            try:
                # ä½¿ç”¨å®‰å…¨è§£æ
                parsed_refs = self._safe_json_loads(refs_str)
                
                # é©—è­‰è§£æçµæœ
                if isinstance(parsed_refs, list):
                    # é©—è­‰æ¯å€‹ reference çš„æ ¼å¼
                    valid_refs = []
                    for ref in parsed_refs:
                        if isinstance(ref, dict) and 'content' in ref:
                            valid_refs.append(ref)
                    
                    if valid_refs:
                        fixed_count += 1
                        return json.dumps(valid_refs, ensure_ascii=False)
                    else:
                        error_count += 1
                        return json.dumps([], ensure_ascii=False)
                else:
                    error_count += 1
                    return json.dumps([], ensure_ascii=False)
                    
            except Exception as e:
                print(f"ä¿®å¾©å¤±æ•—: {e}")
                print(f"åŸå§‹è³‡æ–™: {refs_str[:100]}...")
                error_count += 1
                return json.dumps([], ensure_ascii=False)
        
        print("æ­£åœ¨ä¿®å¾© references æ¬„ä½...")
        df['references'] = df['references'].apply(fix_reference_field)
        
        # ä¿å­˜ä¿®å¾©å¾Œçš„æ–‡ä»¶
        df.to_csv(self.questions_csv_path, index=False)
        
        print(f"âœ… ä¿®å¾©å®Œæˆ!")
        print(f"   æˆåŠŸä¿®å¾©: {fixed_count} æ¢")
        print(f"   ä¿®å¾©å¤±æ•—: {error_count} æ¢")
        print(f"   ç¸½è¨ˆ: {len(df)} æ¢")
        
        if error_count > 0:
            print(f"âš ï¸  æœ‰ {error_count} æ¢è¨˜éŒ„ç„¡æ³•ä¿®å¾©ï¼Œå·²è¨­ç‚ºç©º list")

    def validate_csv_integrity(self):
        """
        é©—è­‰ CSV æ–‡ä»¶çš„å®Œæ•´æ€§
        """
        print("ğŸ” é©—è­‰ CSV æ–‡ä»¶å®Œæ•´æ€§...")
        
        if not os.path.exists(self.questions_csv_path):
            print("âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            if 'corpus_id' in df.columns:
                df['corpus_id'] = df['corpus_id'].str.replace('\\', '/', regex=False)
            df = pd.read_csv(self.questions_csv_path)
            print(f"ğŸ“Š è®€å–åˆ° {len(df)} æ¢è¨˜éŒ„")
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_columns = ['question', 'references', 'corpus_id']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                print(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                return False
            
            # æª¢æŸ¥ references æ¬„ä½
            valid_count = 0
            invalid_count = 0
            
            for idx, refs_str in enumerate(df['references']):
                try:
                    parsed_refs = self._safe_json_loads(refs_str)
                    if isinstance(parsed_refs, list) and len(parsed_refs) > 0:
                        # æª¢æŸ¥ç¬¬ä¸€å€‹ reference çš„æ ¼å¼
                        first_ref = parsed_refs[0]
                        if isinstance(first_ref, dict) and 'content' in first_ref:
                            valid_count += 1
                        else:
                            invalid_count += 1
                            if invalid_count <= 3:  # åªé¡¯ç¤ºå‰3å€‹éŒ¯èª¤ç¤ºä¾‹
                                print(f"âŒ ç¬¬ {idx+1} è¡Œæ ¼å¼éŒ¯èª¤: {first_ref}")
                    else:
                        invalid_count += 1
                        if invalid_count <= 3:
                            print(f"âŒ ç¬¬ {idx+1} è¡Œ references ç‚ºç©ºæˆ–æ ¼å¼éŒ¯èª¤")
                except Exception as e:
                    invalid_count += 1
                    if invalid_count <= 3:
                        print(f"âŒ ç¬¬ {idx+1} è¡Œè§£æå¤±æ•—: {e}")
            
            print(f"âœ… æœ‰æ•ˆè¨˜éŒ„: {valid_count}")
            print(f"âŒ ç„¡æ•ˆè¨˜éŒ„: {invalid_count}")
            
            if invalid_count == 0:
                print("ğŸ‰ CSV æ–‡ä»¶å®Œæ•´æ€§é©—è­‰é€šé!")
                return True
            else:
                print(f"âš ï¸  ç™¼ç¾ {invalid_count} æ¢æœ‰å•é¡Œçš„è¨˜éŒ„")
                return False
                
        except Exception as e:
            print(f"âŒ é©—è­‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False