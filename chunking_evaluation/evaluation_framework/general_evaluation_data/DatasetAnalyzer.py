import os
import re
from collections import defaultdict
from pathlib import Path
import pandas as pd
from bs4 import BeautifulSoup

class DatasetAnalyzer:
    def __init__(self, folder_path: str):
        self.folder_path = folder_path
        self.file_stats = []
        self.category_stats = defaultdict(int)
        
    def _clean_html(self, html_content: str) -> str:
        """æ¸…ç†HTMLå…§å®¹ï¼Œæå–ç´”æ–‡æœ¬"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            # ç§»é™¤scriptå’Œstyleæ¨™ç±¤
            for script_or_style in soup(['script', 'style']):
                script_or_style.decompose()
            # æå–ç´”æ–‡æœ¬
            clean_text = soup.get_text()
            # æ¸…ç†å¤šé¤˜ç©ºç™½
            clean_text = ' '.join(clean_text.split())
            return clean_text
        except Exception as e:
            print(f"HTMLæ¸…ç†å¤±æ•—: {e}")
            return html_content

    def _clean_text(self, text_content: str) -> str:
        """æ¸…ç†ç´”æ–‡æœ¬å…§å®¹"""
        return re.sub(r'\s+', ' ', text_content).strip()

    def _get_file_content_and_word_count(self, file_path: str) -> tuple:
        """è®€å–æª”æ¡ˆå…§å®¹ä¸¦è¨ˆç®—å­—æ•¸"""
        file_extension = os.path.splitext(file_path)[1].lower()
        
        try:
            # å˜—è©¦UTF-8ç·¨ç¢¼
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_content = f.read()
        except UnicodeDecodeError:
            try:
                # å˜—è©¦latin-1ç·¨ç¢¼
                with open(file_path, 'r', encoding='latin-1') as f:
                    raw_content = f.read()
            except Exception as e:
                print(f"âŒ ç„¡æ³•è®€å–æª”æ¡ˆ {file_path}: {e}")
                return "", 0
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return "", 0

        # æ ¹æ“šå‰¯æª”åæ¸…ç†å…§å®¹
        if file_extension in ['.html', '.htm']:
            cleaned_content = self._clean_html(raw_content)
        elif file_extension in ['.txt', '.md', '.markdown']:
            cleaned_content = self._clean_text(raw_content)
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œå˜—è©¦ç•¶ä½œç´”æ–‡æœ¬è™•ç†
            cleaned_content = self._clean_text(raw_content)
        
        # è¨ˆç®—å­—æ•¸ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
        # ä¸­æ–‡å­—ç¬¦è¨ˆæ•¸
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', cleaned_content))
        # è‹±æ–‡å–®è©è¨ˆæ•¸
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', cleaned_content))
        # æ•¸å­—è¨ˆæ•¸
        numbers = len(re.findall(r'\b\d+\b', cleaned_content))
        
        total_word_count = chinese_chars + english_words + numbers
        
        return cleaned_content, total_word_count

    def _extract_category_from_filename(self, filename: str) -> str:
        """å¾æª”åæå–åˆ†é¡ï¼ˆç¬¬äºŒå€‹åº•ç·šåˆ°ç¬¬ä¸‰å€‹åº•ç·šï¼‰"""
        try:
            # ç§»é™¤å‰¯æª”å
            name_without_ext = os.path.splitext(filename)[0]
            # ä»¥åº•ç·šåˆ†å‰²
            parts = name_without_ext.split('_')
            
            if len(parts) >= 4:
                # æå–ç¬¬äºŒå€‹åº•ç·šåˆ°ç¬¬ä¸‰å€‹åº•ç·šçš„éƒ¨åˆ†ï¼ˆç´¢å¼•2ï¼‰
                category = parts[2]
                return category
            else:
                return "æœªåˆ†é¡"
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è§£ææª”ååˆ†é¡ {filename}: {e}")
            return "è§£æéŒ¯èª¤"

    def analyze_folder(self):
        """åˆ†ææ•´å€‹è³‡æ–™å¤¾"""
        print(f"ğŸ” é–‹å§‹åˆ†æè³‡æ–™å¤¾: {self.folder_path}")
        print("="*60)
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆè·¯å¾‘
        folder_path_obj = Path(self.folder_path)
        if not folder_path_obj.exists():
            print(f"âŒ è³‡æ–™å¤¾ä¸å­˜åœ¨: {self.folder_path}")
            return
        
        # æ‰¾å‡ºæ‰€æœ‰æª”æ¡ˆ
        all_files = list(folder_path_obj.rglob('*'))
        file_paths = [f for f in all_files if f.is_file()]
        
        total_files = len(file_paths)
        print(f"ğŸ“ æ‰¾åˆ° {total_files} å€‹æª”æ¡ˆ")
        print()
        
        # é€ä¸€è™•ç†æ¯å€‹æª”æ¡ˆ
        for i, file_path in enumerate(file_paths, 1):
            self._process_single_file(file_path, i, total_files)
            
            # æ¯è™•ç†50å€‹æª”æ¡ˆé¡¯ç¤ºä¸€æ¬¡é€²åº¦
            if i % 50 == 0:
                print(f"ğŸ“Š å·²è™•ç† {i}/{total_files} å€‹æª”æ¡ˆ ({i/total_files*100:.1f}%)")
        
        print(f"\nâœ… è™•ç†å®Œæˆï¼ç¸½å…±è™•ç†äº† {total_files} å€‹æª”æ¡ˆ")
        
    def _process_single_file(self, file_path: Path, current_idx: int, total_files: int):
        """è™•ç†å–®ä¸€æª”æ¡ˆ"""
        try:
            filename = file_path.name
            file_extension = file_path.suffix.lower()
            file_size = file_path.stat().st_size
            
            # é¡¯ç¤ºç•¶å‰è™•ç†çš„æª”æ¡ˆ
            if current_idx % 10 == 1:  # æ¯10å€‹æª”æ¡ˆé¡¯ç¤ºä¸€æ¬¡
                print(f"ğŸ“„ [{current_idx}/{total_files}] è™•ç†: {filename}")
            
            # æå–åˆ†é¡
            category = self._extract_category_from_filename(filename)
            self.category_stats[category] += 1
            
            # è®€å–å…§å®¹ä¸¦è¨ˆç®—å­—æ•¸
            content, word_count = self._get_file_content_and_word_count(str(file_path))
            
            # è¨˜éŒ„æª”æ¡ˆçµ±è¨ˆè³‡è¨Š
            file_info = {
                'filename': filename,
                'file_path': str(file_path),
                'file_extension': file_extension,
                'file_size_bytes': file_size,
                'file_size_kb': round(file_size / 1024, 2),
                'word_count': word_count,
                'category': category,
                'content_length': len(content)
            }
            
            self.file_stats.append(file_info)
            
        except Exception as e:
            print(f"âŒ è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ {file_path}: {e}")

    def generate_report(self):
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        if not self.file_stats:
            print("âŒ æ²’æœ‰å¯ç”¨çš„çµ±è¨ˆè³‡æ–™")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š è³‡æ–™é›†åˆ†æå ±å‘Š")
        print("="*80)
        
        # åŸºæœ¬çµ±è¨ˆ
        total_files = len(self.file_stats)
        print(f"ğŸ“ æª”æ¡ˆç¸½æ•¸: {total_files}")
        
        # æª”æ¡ˆæ ¼å¼çµ±è¨ˆ
        print("\nğŸ¯ æª”æ¡ˆæ ¼å¼åˆ†ä½ˆ:")
        format_stats = defaultdict(int)
        for file_info in self.file_stats:
            format_stats[file_info['file_extension']] += 1
        
        for ext, count in sorted(format_stats.items()):
            percentage = count / total_files * 100
            print(f"   {ext or 'ç„¡å‰¯æª”å'}: {count} å€‹ ({percentage:.1f}%)")
        
        # æª”æ¡ˆå¤§å°çµ±è¨ˆ
        print("\nğŸ“ æª”æ¡ˆå¤§å°çµ±è¨ˆ:")
        sizes = [info['file_size_kb'] for info in self.file_stats]
        print(f"   å¹³å‡å¤§å°: {sum(sizes)/len(sizes):.2f} KB")
        print(f"   æœ€å¤§æª”æ¡ˆ: {max(sizes):.2f} KB")
        print(f"   æœ€å°æª”æ¡ˆ: {min(sizes):.2f} KB")
        
        # å­—æ•¸çµ±è¨ˆ
        print("\nğŸ“ å­—æ•¸çµ±è¨ˆ:")
        word_counts = [info['word_count'] for info in self.file_stats]
        print(f"   å¹³å‡å­—æ•¸: {sum(word_counts)/len(word_counts):.0f} å­—")
        print(f"   æœ€å¤šå­—æ•¸: {max(word_counts):,} å­—")
        print(f"   æœ€å°‘å­—æ•¸: {min(word_counts):,} å­—")
        print(f"   ç¸½å­—æ•¸: {sum(word_counts):,} å­—")
        
        # åˆ†é¡çµ±è¨ˆï¼ˆæŒ‰æª”æ¡ˆæ•¸é‡æ’åºï¼‰
        print(f"\nğŸ« åˆ†é¡çµ±è¨ˆ (å…± {len(self.category_stats)} å€‹åˆ†é¡):")
        sorted_categories = sorted(self.category_stats.items(), key=lambda x: x[1], reverse=True)
        
        for category, count in sorted_categories:
            percentage = count / total_files * 100
            print(f"   {category}: {count} ç­† ({percentage:.1f}%)")
        
        print("\n" + "="*80)

    def save_detailed_report(self, output_path: str = "dataset_analysis_report.csv"):
        """ä¿å­˜è©³ç´°å ±å‘Šåˆ°CSVæª”æ¡ˆ"""
        if not self.file_stats:
            print("âŒ æ²’æœ‰å¯ç”¨çš„çµ±è¨ˆè³‡æ–™")
            return
        
        try:
            df = pd.DataFrame(self.file_stats)
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"âœ… è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å ±å‘Šå¤±æ•—: {e}")

    def save_category_summary(self, output_path: str = "category_summary.csv"):
        """ä¿å­˜åˆ†é¡æ‘˜è¦åˆ°CSVæª”æ¡ˆ"""
        if not self.category_stats:
            print("âŒ æ²’æœ‰åˆ†é¡çµ±è¨ˆè³‡æ–™")
            return
        
        try:
            # æº–å‚™åˆ†é¡æ‘˜è¦è³‡æ–™
            category_data = []
            total_files = len(self.file_stats)
            
            for category, count in sorted(self.category_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = count / total_files * 100
                category_data.append({
                    'category': category,
                    'file_count': count,
                    'percentage': round(percentage, 2)
                })
            
            df = pd.DataFrame(category_data)
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"âœ… åˆ†é¡æ‘˜è¦å·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†é¡æ‘˜è¦å¤±æ•—: {e}")

    def get_stratified_sample(self, total_sample_size: int = 100, min_per_category: int = 1):
        """æ ¹æ“šåˆ†é¡æ¯”ä¾‹é€²è¡Œåˆ†å±¤æŠ½æ¨£"""
        if not self.file_stats:
            print("âŒ æ²’æœ‰å¯ç”¨çš„çµ±è¨ˆè³‡æ–™")
            return []
        
        print(f"\nğŸ¯ é€²è¡Œåˆ†å±¤æŠ½æ¨£ (ç›®æ¨™æ¨£æœ¬æ•¸: {total_sample_size})")
        
        # æŒ‰åˆ†é¡åˆ†çµ„æª”æ¡ˆ
        files_by_category = defaultdict(list)
        for file_info in self.file_stats:
            files_by_category[file_info['category']].append(file_info)
        
        # è¨ˆç®—æ¯å€‹åˆ†é¡æ‡‰è©²æŠ½å–çš„æ•¸é‡
        total_files = len(self.file_stats)
        sampled_files = []
        remaining_sample_size = total_sample_size
        
        sorted_categories = sorted(self.category_stats.items(), key=lambda x: x[1], reverse=True)
        
        for i, (category, count) in enumerate(sorted_categories):
            if remaining_sample_size <= 0:
                break
                
            # è¨ˆç®—è©²åˆ†é¡çš„æŠ½æ¨£æ•¸é‡
            if i == len(sorted_categories) - 1:  # æœ€å¾Œä¸€å€‹åˆ†é¡ï¼Œç”¨å‰©é¤˜çš„å…¨éƒ¨
                sample_size = remaining_sample_size
            else:
                # æŒ‰æ¯”ä¾‹è¨ˆç®—
                proportion = count / total_files
                sample_size = max(min_per_category, int(total_sample_size * proportion))
                sample_size = min(sample_size, count, remaining_sample_size)
            
            # å¾è©²åˆ†é¡éš¨æ©ŸæŠ½æ¨£
            import random
            category_files = files_by_category[category]
            if len(category_files) >= sample_size:
                selected = random.sample(category_files, sample_size)
            else:
                selected = category_files  # å¦‚æœæª”æ¡ˆæ•¸ä¸è¶³ï¼Œå…¨éƒ¨é¸å–
            
            sampled_files.extend(selected)
            remaining_sample_size -= len(selected)
            
            print(f"   {category}: {len(selected)}/{count} å€‹æª”æ¡ˆ")
        
        print(f"\nâœ… å®Œæˆåˆ†å±¤æŠ½æ¨£ï¼Œå…±æŠ½å– {len(sampled_files)} å€‹æª”æ¡ˆ")
        return sampled_files

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # æŒ‡å®šè¦åˆ†æçš„è³‡æ–™å¤¾è·¯å¾‘
    folder_path = "./corpora"
    
    # å‰µå»ºåˆ†æå™¨
    analyzer = DatasetAnalyzer(folder_path)
    
    # åŸ·è¡Œåˆ†æ
    analyzer.analyze_folder()
    
    # ç”Ÿæˆå ±å‘Š
    analyzer.generate_report()
    
    # ä¿å­˜è©³ç´°å ±å‘Š
    analyzer.save_detailed_report("dataset_analysis_detailed.csv")
    analyzer.save_category_summary("dataset_analysis_categories.csv")
    
    # é€²è¡Œåˆ†å±¤æŠ½æ¨£
    sample = analyzer.get_stratified_sample(total_sample_size=100)
    
    # ä¿å­˜æŠ½æ¨£çµæœ
    if sample:
        sample_df = pd.DataFrame(sample)
        sample_df.to_csv("stratified_sample_100.csv", index=False, encoding='utf-8-sig')
        print("âœ… åˆ†å±¤æŠ½æ¨£çµæœå·²ä¿å­˜åˆ°: stratified_sample_100.csv")